{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "In Q2, we will build a convolution neural neural net with two convolution layers. For the tutorial on the convolution neural networks, please check the [link](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read in data\n",
    "Use tf learn's built-in function to load MNIST data from\n",
    "the folder MNIST_data.\n",
    "\n",
    "HINT: Use `input_data.read_data_sets()` as in Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "mnist = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define paramaters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create placeholders for data points and labels\n",
    "HINT: use `tf.placeholder` as in Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "x = \n",
    "y = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Reshape x\n",
    "\n",
    "Convolution is an operator for 2d images. Therefore, we reshape the flattened image to a 2d array. The shape of x is (batch_size, height * width). Convert `x` to `x_image` which is a 4-d tensor of the shape (batch_size, height, width, channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create the first convolution layer\n",
    "Let's build our first convolution layer. [5, 5, 1, 32] stands for [filter_height, filter_width, in_channels, out_channels]. The number of `in_channels` is 1, since we have grayscale images, thus only 1 color channel. We will be computing 32 features for each 5x5 patch, hence 32 `out_channels`.\n",
    "\n",
    "The weights are randomly initialized following a normal distribution. Don't forget to add the bias term to the convolution layer. Then, we feed the output of the convolution layer into the activation function. We use a [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) activation layer. It is followed by the [max-pooling](https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer) layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1)) #truncated normal distr to avoid extreme values\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape=[32])) #slightly positive bias works better with ReLU\n",
    "h_conv1 = tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1\n",
    "h_activate1 = tf.nn.relu(h_conv1)\n",
    "h_pool1 = tf.nn.max_pool(h_activate1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "#Note: the 4 dimensions of strides and ksize correspond to each dimension of the input tensor image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create the second convolution layer\n",
    "We need to implement another convolution layer. The filter size is also 5x5 and the number of `out_channels` chosen is 64.\n",
    "\n",
    "HINT: refer to Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "W_conv2 = \n",
    "b_conv2 = \n",
    "h_conv2 = \n",
    "h_activate2 = \n",
    "h_pool2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Flatten h_pool2\n",
    "The shape of `h_pool2` will be [None, 7, 7, 64] (`None`, as usual, stands for an arbitrary batch size, but do you understand why we get 7x7x64?).\n",
    "\n",
    "Now, we need to flatten `h_pool2` to the shape (-1, 7\\*7\\*64) because later it will be passed on to fully connected layer.\n",
    "\n",
    "HINT: use `tf.reshape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "h_pool2_flat = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Fully connected layer\n",
    "The fully connected layer will have 1024 units and is very similar to step 4\n",
    "in Q1. However, you need to feed the output of this layer to a ReLU\n",
    "activation layer rather than the softmax cross entropy.\n",
    "\n",
    "HINT: use `tf.nn.relu` and `tf.matmul`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "W_fc1 = \n",
    "b_fc1 = \n",
    "h_fc1 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Dropout layer \n",
    "[Dropout](https://en.wikipedia.org/wiki/Convolutional_neural_network#Dropout) is used to regularize the neural network during training. The variable `dropout` is defined as a placeholder, so we need to pass its value to the model later (the value is the probability that a neuron's output is kept after the dropout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout = tf.placeholder(tf.float32, name='dropout')\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Readout layer\n",
    "We add another fully connected layer. This time, the output size is 10, which is the number of classes. Don't forget to add the bias term! The output serves actually as logits (do remember the logits in Q1?).\n",
    "\n",
    "HINT: refer to Step 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "W_fc2 = \n",
    "b_fc2 = \n",
    "logits = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Define the loss function\n",
    "Compute the mean cross entropy loss with the logits and the labels. Notice that this final layer is exactly analogous to the single \"layer\" we had in Q1.\n",
    "\n",
    "HINT: use `softmax_cross_entropy_with_logits`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "loss = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Optimization\n",
    "To optimize the neural net, we will use the Adam algorithm. What is the Adam? It is a gradient-based optimization algorithm. For details, check this [link](http://ruder.io/optimizing-gradient-descent/).\n",
    "\n",
    "HINT: use `tf.train.AdamOptimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "# initialize\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(3000):\n",
    "    batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "    if i % 100 == 0:#compute training accuracy after every 100 batches\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={\n",
    "            x: batch[0], y: batch[1], dropout: 1.0})\n",
    "        print('step {}, training accuracy {}'.format(i, train_accuracy))\n",
    "        \n",
    "    # TODO: run train_step. Don't forget to include the dropout placeholder in the feed_dict! (set it to 0.75)\n",
    "    sess.run(\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('test accuracy {}'.format(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, dropout: 1.0})))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
