{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "In Q2, we will build a convolution neural neural net with two convolution layers. For the tutorial on the convolution neural networks, please check the [link](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I had several errors (that I found because of shape errors)\n",
    "# tf.get_shape is a wonderful function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read in data\n",
    "Use tf learn's built-in function to load MNIST data from\n",
    "the folder MNIST_data.\n",
    "\n",
    "HINT: Use `input_data.read_data_sets()` as in Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define paramaters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create placeholders for data points and labels\n",
    "HINT: use `tf.placeholder` as in Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784], name='x')\n",
    "y = tf.placeholder(tf.int32, [None, 10], name='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Reshape x\n",
    "\n",
    "Convolution is an operator for 2d images. Therefore, we reshape the flattened image to a 2d array. The shape of x is (batch_size, height * width). Convert `x` to `x_image` which is a 4-d tensor of the shape (batch_size, height, width, channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create the first convolution layer\n",
    "Let's build our first convolution layer. [5, 5, 1, 32] stands for [filter_height, filter_width, in_channels, out_channels]. The number of `in_channels` is 1, since we have grayscale images, thus only 1 color channel. We will be computing 32 features for each 5x5 patch, hence 32 `out_channels`.\n",
    "\n",
    "The weights are randomly initialized following a normal distribution. Don't forget to add the bias term to the convolution layer. Then, we feed the output of the convolution layer into the activation function. We use a [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) activation layer. It is followed by the [max-pooling](https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer) layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1)) #truncated normal distr to avoid extreme values\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape=[32])) #slightly positive bias works better with ReLU\n",
    "h_conv1 = tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1\n",
    "h_activate1 = tf.nn.relu(h_conv1)\n",
    "h_pool1 = tf.nn.max_pool(h_activate1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "#Note: the 4 dimensions of strides and ksize correspond to each dimension of the input tensor image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create the second convolution layer\n",
    "We need to implement another convolution layer. The filter size is also 5x5 and the number of `out_channels` chosen is 64.\n",
    "\n",
    "HINT: refer to Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_conv2 = tf.Variable(tf.truncated_normal([5,5,32,64], stddev=0.1))\n",
    "b_conv2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "# take input from the previous pooling layer:\n",
    "h_conv2 = tf.nn.conv2d(h_pool1, W_conv2, strides=[1,1,1,1], padding='SAME') + b_conv2\n",
    "h_activate2 = tf.nn.relu(h_conv2)\n",
    "h_pool2 = tf.nn.max_pool(h_activate2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Flatten h_pool2\n",
    "The shape of `h_pool2` will be [None, 7, 7, 64] (`None`, as usual, stands for an arbitrary batch size, but do you understand why we get 7x7x64?).\n",
    "\n",
    "Now, we need to flatten `h_pool2` to the shape (-1, 7\\*7\\*64) because later it will be passed on to fully connected layer.\n",
    "\n",
    "HINT: use `tf.reshape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Interestingly -1 is cannot be exchanged for None like in tf.placeholder\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Fully connected layer\n",
    "The fully connected layer will have 1024 units and is very similar to step 4\n",
    "in Q1. However, you need to feed the output of this layer to a ReLU\n",
    "activation layer rather than the softmax cross entropy.\n",
    "\n",
    "HINT: use `tf.nn.relu` and `tf.matmul`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "W_fc1 = tf.Variable(tf.random_normal(shape=[7*7*64,1024], stddev=0.01), name=\"dense-1-weights\")\n",
    "b_fc1 = tf.Variable(tf.zeros([1,1024]), name=\"dense-1-bias\")\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Dropout layer \n",
    "[Dropout](https://en.wikipedia.org/wiki/Convolutional_neural_network#Dropout) is used to regularize the neural network during training. The variable `dropout` is defined as a placeholder, so we need to pass its value to the model later (the value is the probability that a neuron's output is kept after the dropout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout = tf.placeholder(tf.float32, name='dropout')\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Readout layer\n",
    "We add another fully connected layer. This time, the output size is 10, which is the number of classes. Don't forget to add the bias term! The output serves actually as logits (do remember the logits in Q1?).\n",
    "\n",
    "HINT: refer to Step 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "W_fc2 = tf.Variable(tf.random_normal(shape=[1024,10], stddev=0.01), name=\"dense-2-weights\")\n",
    "b_fc2 = tf.Variable(tf.zeros([1,10]), name=\"dense-2-bias\")\n",
    "logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Define the loss function\n",
    "Compute the mean cross entropy loss with the logits and the labels. Notice that this final layer is exactly analogous to the single \"layer\" we had in Q1.\n",
    "\n",
    "HINT: use `softmax_cross_entropy_with_logits`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y, name='loss')\n",
    "loss = entropy#tf.reduce_mean(entropy) <- Don't want to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Optimization\n",
    "To optimize the neural net, we will use the Adam algorithm. What is the Adam? It is a gradient-based optimization algorithm. For details, check this [link](http://ruder.io/optimizing-gradient-descent/).\n",
    "\n",
    "HINT: use `tf.train.AdamOptimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.1875\n",
      "step 100, training accuracy 0.78125\n",
      "step 200, training accuracy 0.78125\n",
      "step 300, training accuracy 1.0\n",
      "step 400, training accuracy 0.9375\n",
      "step 500, training accuracy 0.84375\n",
      "step 600, training accuracy 1.0\n",
      "step 700, training accuracy 0.96875\n",
      "step 800, training accuracy 0.875\n",
      "step 900, training accuracy 0.84375\n",
      "step 1000, training accuracy 0.90625\n",
      "step 1100, training accuracy 0.96875\n",
      "step 1200, training accuracy 0.96875\n",
      "step 1300, training accuracy 1.0\n",
      "step 1400, training accuracy 1.0\n",
      "step 1500, training accuracy 0.90625\n",
      "step 1600, training accuracy 1.0\n",
      "step 1700, training accuracy 0.96875\n",
      "step 1800, training accuracy 0.9375\n",
      "step 1900, training accuracy 0.9375\n",
      "step 2000, training accuracy 1.0\n",
      "step 2100, training accuracy 0.96875\n",
      "step 2200, training accuracy 0.96875\n",
      "step 2300, training accuracy 0.96875\n",
      "step 2400, training accuracy 1.0\n",
      "step 2500, training accuracy 1.0\n",
      "step 2600, training accuracy 1.0\n",
      "step 2700, training accuracy 0.96875\n",
      "step 2800, training accuracy 1.0\n",
      "step 2900, training accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "# initialize\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(3000):\n",
    "    batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "    if i % 100 == 0:#compute training accuracy after every 100 batches\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={\n",
    "            x: batch[0], y: batch[1], dropout: 1.0})\n",
    "        print('step {}, training accuracy {}'.format(i, train_accuracy))\n",
    "        \n",
    "    # TODO: run train_step. Don't forget to include the dropout placeholder in the feed_dict! (set it to 0.75)\n",
    "    sess.run(train_step, feed_dict={\n",
    "            x: batch[0], y: batch[1], dropout: 0.75})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.981599986553\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('test accuracy {}'.format(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, dropout: 1.0})))\n",
    "except tf.errors.InvalidArgumentError as e:\n",
    "    print(e.message)\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
