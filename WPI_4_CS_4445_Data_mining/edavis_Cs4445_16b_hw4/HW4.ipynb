{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 : MapReduce and Deep Learning (100 points)\n",
    "\n",
    "*------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** NOTE **\n",
    "* Please don't forget to save the notebook frequently when working in IPython Notebook, otherwise the changes you made can be lost.\n",
    "\n",
    "*----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required reading:\n",
    "* [Reinforcement learning](http://karpathy.github.io/2016/05/31/rl/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 (50 points): Using MapReduce framework to implement Matrix Multiplication\n",
    "In this question, you need to use your editor to work on your code, instead of iPython notebook.\n",
    "In the folder, we have a file **input_matrices.data** for the input matrices A and B. \n",
    "The goal is to create a file, named **mr_matrix_multiplication.py**, to compute the matrix C = A X B.\n",
    "In this file, you should use the MapReduce framework to solve the problem, by implementing a *map* function and *reduce* function.\n",
    "\n",
    "Please run your code through the terminal and save the output results into a file named **output**.\n",
    "For example, in linux/mac, you could use the following command in the terminal:\n",
    "\n",
    " *cat input_matrices.data | python mr_matrix_multiplication.py > output*\n",
    " \n",
    " Note in this question, you cannot use any matrix multiplication function in existing packages, such as those in numpy and scipy. The output file should use a format that is similar to the format of the file *input_matrices.data*. Each line of the file represents one element of Matrix C. \n",
    " For example, any of the following formats would be good.\n",
    "* C, 1, 1, 132 \n",
    "* \"C\", 1, 1, 132 \n",
    "* \"C\" (1, 1, 132)\n",
    "* _ (\"C\", 1, 1, 132)\n",
    "\n",
    "Here 132 is just a made-up number, which may not be the correct answer for C[1,1].\n",
    "We assume the indices of the matrix elements start from 1, instead of 0. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (50 points): Reinforcement Learning for Pacman\n",
    "We first introduce an example of how to use the gym package from OpenAI to design an agent for Pacman Game.\n",
    "In the following cell, we implemented a simple agent, which randomly picks the next action without looking at the screen image (i.e., the *observation*) or the reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the following code for **myAgent** to design a better agent, which takes the observation and reward as the input and picks the best action as the next move.\n",
    "The agent should be able to improve itself after playing more games.\n",
    "\n",
    "***The goals***: Implement an agent using neural networks that can achieve all the following goals:\n",
    "* (a) move the PacMan in all directions.  (10 points)\n",
    "* (b) using neural network to decide what is the best next move. (20 points)\n",
    "* (c) after playing each episode of the game, the agent should be able to improve itself using the experience. (20 points)\n",
    "\n",
    "Action Code:\n",
    "* 1 - UP\n",
    "* 2 - RIGHT\n",
    "* 3 - LEFT\n",
    "* 4 - DOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "up = 1\n",
    "right = 2\n",
    "left = 3\n",
    "down = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-28 12:07:45,268] Making new env: MsPacman-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.23555067 -0.52892515  0.04622637  0.06644255  0.4893885  -0.21786299\n",
      "  0.51827246 -0.74510926 -0.64666406 -0.55194508  0.10331666 -0.38398356\n",
      " -1.1155393   0.77461355  0.16881577  0.17468138  0.17127192 -0.08063927\n",
      "  0.18122813 -0.18760231  0.36394204  0.72068462 -0.64955361  0.36055983\n",
      "  0.34656904 -0.16239621 -0.41108046 -0.35363563  0.2720348   0.88434355\n",
      " -0.91936528 -0.32042209 -0.19193076 -0.14533906  0.0092817   0.59916981\n",
      " -0.17376264 -0.23022782 -1.12475335 -0.24116079  0.25629914  0.68494555\n",
      " -0.02133206  0.72409059  0.48305808 -0.97074328 -0.07848461 -0.04440541\n",
      " -0.57985543  0.02487741 -0.26313218  0.4939594  -0.5244822   0.60255651\n",
      "  0.28271841 -0.15355441  0.76851258  0.51783137 -0.57699956 -0.38264111\n",
      " -0.92104838  0.509549   -0.22035628  0.57875807  0.68203606  0.17254183\n",
      " -0.40066786 -0.10391948 -0.00962682  0.57574893 -0.69275117  0.30442946\n",
      " -0.06236149 -1.27758099 -0.03902068 -0.15475187 -0.28618196  0.68710144\n",
      " -0.0400176  -0.2554607  -0.4253331  -0.31363347 -0.63062081  0.48598276\n",
      "  0.14664573  0.87226554 -0.28551343 -0.20823316 -0.12872436  0.20248493\n",
      " -0.67847248 -0.06196276  0.54993464 -0.51122445 -0.19011884 -0.08725154\n",
      " -0.32431221 -0.35598192  0.08352504  1.08988446  0.42178844  0.19134087\n",
      " -0.76241822 -0.13773643 -0.04421826  0.27991704 -0.26540218  0.15131342\n",
      "  0.75989955 -0.30487568  0.37965356  0.09444009  0.96501396  0.73256418\n",
      "  0.68783173 -0.52283629  0.75772166 -0.08098537 -0.14857044  0.15759584\n",
      " -0.49495821  0.22646044  0.33786283  0.41689737 -0.47329095 -0.9306028\n",
      " -0.64219705 -0.23137531  0.55970578 -0.29697108 -0.81265262 -0.20182719\n",
      "  0.25301944  0.51660778 -0.45765171  0.18821741  0.16067673  0.5871181\n",
      "  0.84810062 -0.18985361  0.56444341 -0.4090887  -0.55351617 -0.47065531\n",
      "  0.26139047 -0.34672391 -0.1192666   0.37130345 -0.43713807 -0.21120132\n",
      " -0.60741406 -1.17989829  0.03879015  0.79087164  0.00611652  0.31040926\n",
      "  0.03910408 -0.35607877  0.2303528   0.26134395  0.40881942  0.12220054\n",
      "  0.31286877 -0.5001706  -0.91807698  0.1302998  -0.13859532  0.43989698\n",
      "  0.40330086 -0.19989573 -0.25146007  1.05367072  0.70595316 -0.41134758\n",
      "  0.72307949 -0.44227357  0.08197567  0.51721318 -0.31121983  0.92188719\n",
      " -0.14084989 -0.76707565 -0.08893167 -0.2534032  -0.59000119 -0.29296664\n",
      " -0.72511658 -0.47035377  0.07317457 -0.05850809  0.70698162 -0.72543142\n",
      " -1.04902673  0.24225585 -0.54494633  0.14699186 -0.20355175 -0.16386991\n",
      " -0.00928921 -0.55947393]\n",
      "resetting env. episode reward total was 220.000000. running mean: 220.000000\n",
      "[ -1.14530074e-01   7.85713676e-02  -6.24672583e-02  -2.31711689e-01\n",
      "  -7.20629422e-01   1.36988939e-01  -2.72254097e-01  -5.20711362e-02\n",
      "   1.12496211e+00   2.92465748e-01  -2.12884345e-01  -2.22878153e-01\n",
      "   5.31508413e-01  -4.12188088e-01  -2.52847974e-01   1.24099182e-01\n",
      "  -1.29431867e+00   4.03370370e-01  -8.66112841e-03  -1.16903570e-01\n",
      "  -7.01045817e-01  -2.21404459e-02   6.32953985e-01  -7.26038276e-01\n",
      "   8.84744042e-01   4.29485751e-01   2.44706968e-03  -3.86061421e-02\n",
      "   1.72915287e-01   2.53124285e-01  -5.97854949e-04   5.65141146e-01\n",
      "  -5.63847333e-01  -1.02575254e+00  -2.26154370e-02   1.82519760e-01\n",
      "   3.04087688e-01   4.18957538e-01   7.10940682e-01   4.01491895e-01\n",
      "   3.20826148e-01  -4.69122658e-03  -1.06493079e+00   2.33771674e-02\n",
      "   5.74201542e-01   1.24443962e-01  -1.46680330e-01  -3.97858748e-01\n",
      "   1.08069359e+00   3.77142240e-01   7.77074734e-01  -2.53818120e-02\n",
      "  -2.52300943e-02   6.31419859e-02  -8.64356172e-01  -2.23483923e-01\n",
      "  -1.13318788e+00   5.65109364e-01   6.10893563e-01  -3.06171178e-01\n",
      "  -5.25839987e-01   1.35351138e-01  -1.15762739e+00  -3.38762189e-01\n",
      "  -2.35064779e-01  -6.76903312e-01  -8.66563589e-02   6.04083054e-01\n",
      "  -5.60084035e-01  -1.02164023e+00  -6.30792760e-01   3.12890709e-01\n",
      "   9.08930363e-01   2.65173876e-01   2.28433984e-01  -3.73118956e-01\n",
      "   4.82932972e-01   5.60787528e-01  -7.12269117e-01   4.38916541e-01\n",
      "   1.04795035e+00  -1.04074088e-01  -1.07855738e-02  -8.61252976e-01\n",
      "  -1.62331922e-01  -3.31749606e-01  -3.11944462e-01  -4.70402312e-01\n",
      "   1.36122721e-01  -8.96824267e-01   1.16285613e-01  -5.17842250e-02\n",
      "   9.75765982e-02  -2.62273425e-01  -6.06888833e-01   2.07800562e-01\n",
      "  -2.33545711e-01  -7.63997069e-01  -1.44167910e-01   4.57076754e-02\n",
      "   7.00732660e-01  -1.08220545e-01   4.73983891e-01   1.57573229e-01\n",
      "  -6.81195231e-01  -7.41754303e-01  -3.07778348e-01  -4.49545370e-01\n",
      "  -4.93423680e-01  -3.58010169e-01   2.28617449e-01  -4.03500149e-01\n",
      "   1.10732201e-01   3.16364155e-01  -6.00554821e-01  -2.10173284e-01\n",
      "  -5.39629988e-01   2.15535757e-01  -9.45489519e-01  -1.51087422e+00\n",
      "  -1.78723998e-03  -5.20587872e-01  -8.80708002e-01  -8.00345966e-01\n",
      "  -6.87906583e-01   2.08696112e-01   7.24249752e-01   2.11718563e-01\n",
      "  -5.58922580e-01   5.15806926e-01  -4.69421351e-01  -3.05748805e-02\n",
      "   6.83280274e-01  -3.44574375e-01  -5.05066869e-01  -7.04899134e-01\n",
      "  -1.10427831e-01  -7.33689413e-01  -5.48696289e-01   1.31093953e-01\n",
      "  -1.00286345e-01   1.27821269e+00  -1.65903427e-01  -2.03420828e-01\n",
      "  -4.38672177e-02  -2.75407772e-01  -1.49067746e+00   8.43467173e-02\n",
      "   6.79614786e-02  -9.52880699e-01   5.08345200e-03   1.47628212e+00\n",
      "   1.64069329e-01  -5.42672959e-01   4.56570901e-01  -8.41097252e-01\n",
      "  -6.24024589e-03   3.05507279e-01  -1.11680803e+00   2.38289901e-01\n",
      "   2.41831445e-01   5.52049224e-02  -3.15890794e-01  -9.50893654e-01\n",
      "   9.49208418e-01   4.40891049e-01   6.76584150e-01  -5.54140178e-01\n",
      "  -4.12334666e-01  -1.12178024e-01   1.75645682e-02  -1.16352548e+00\n",
      "  -5.92729931e-02  -6.55456617e-01   4.83662142e-01  -1.06839343e+00\n",
      "  -7.06804036e-01  -1.81150760e-01  -3.55049752e-01   1.04228634e-01\n",
      "  -6.86953937e-01  -7.30299081e-02   4.98337074e-01   3.74354679e-01\n",
      "   7.21964484e-01   8.73683639e-01   2.52583614e-01  -8.53029111e-01\n",
      "   1.73266525e-01   6.56457600e-01  -6.62542036e-01   7.94733207e-02\n",
      "   1.14125419e-01  -5.32655154e-01   6.42047276e-01  -6.38414716e-01\n",
      "  -8.78077088e-01  -1.93083910e-01  -1.92706547e-01  -1.75430385e-01]\n",
      "resetting env. episode reward total was 110.000000. running mean: 218.900000\n",
      "[-0.22428994 -0.8395508  -0.20270354 -0.07459506  0.16837436 -0.14981488\n",
      "  0.11173672 -0.25480722  0.68869485 -0.40262081  0.3279889   0.37757516\n",
      "  0.0866083   0.29831764  0.3770074   0.60330536  0.20008988  0.42125549\n",
      " -0.28804787  0.05565707  1.10467242 -0.31522879 -0.21406277 -0.21668791\n",
      "  0.55777307  0.25107904 -0.45695926  0.05753077 -0.30904475  0.65719021\n",
      " -1.31358844  0.35109455 -0.32521561 -0.63841153 -0.20716283 -0.35446701\n",
      "  0.48151053 -0.30054734 -0.21465811  1.02949756 -0.2072028  -0.52475403\n",
      "  0.23373033  0.22049408  0.51535224  0.29349335 -0.19802456 -0.47649789\n",
      " -0.76681744 -0.76081117 -0.13821311 -0.37538494  1.0108102  -0.01592945\n",
      "  0.09322522 -0.33190345 -0.67709655 -0.36629225  0.63290925  1.2607635\n",
      "  0.51039213 -0.42287728 -0.40074247 -0.00769282  0.06841882 -0.15989177\n",
      "  0.44387759 -0.07978793  0.06995584 -0.48096728 -0.69154288 -0.5137756\n",
      "  0.43322982  0.27953824  0.35837202  0.47379374  0.31441964  0.05598209\n",
      "  0.75545419 -0.4955963   0.13263805 -0.11038544 -0.22822162  1.29524215\n",
      " -0.6909702   0.22208658 -0.09793301 -0.0895106  -0.77923283  1.14825709\n",
      "  0.8936894  -0.42994649  0.00673577 -0.39387854  0.64324753  0.16393468\n",
      "  0.18874667  0.36315135 -0.57035388 -0.34728207 -0.14756741 -0.35188341\n",
      " -0.54747095  0.11400028 -0.20373425  0.39569663  0.2161618   0.26135754\n",
      "  0.52457067  0.4009646  -0.06480912 -0.85921769  0.38919932  1.26687031\n",
      "  0.33807032 -0.87797144 -0.30416247  0.14877541 -0.54069459  0.62458178\n",
      "  0.19714006  0.93179914  0.6554873   0.12670738  0.24476548  0.13199483\n",
      " -0.54423802 -1.28618212 -0.40606193 -0.4467795   0.39851423 -0.02673797\n",
      "  0.95015097  0.07853643  0.48320045 -0.31121732 -0.27277772  0.70341885\n",
      " -0.64378943 -0.35723228  0.55825787 -0.51884615  0.07589317  0.04976756\n",
      "  0.46711659  0.28949592  0.21861821 -0.09036629  0.20317082  0.61586122\n",
      "  0.24429217 -0.16188032 -0.24344064  0.61337024  0.4040501   0.48377018\n",
      "  0.47883804 -0.70510923  0.27343652  0.21356646 -0.55866325  0.85402\n",
      "  0.16386793 -0.30362195  0.14379054  0.38409827 -0.25399957  0.03617708\n",
      " -0.05411975 -0.55571534 -0.14315201  0.49345607  0.30026284 -1.34143658\n",
      "  1.07668209  0.51678583 -1.19695661  0.37619886  0.86165893 -0.90322357\n",
      "  0.73634633  0.08594771 -0.41420682 -1.46632881 -1.16548428 -0.13487858\n",
      " -0.87915    -0.32875826 -0.52025742  0.46682979 -0.32701205  0.576822\n",
      "  0.63838658 -0.31288268  0.08573319  0.02009959  1.34914664 -0.09707458\n",
      "  0.65083336  0.9965129 ]\n",
      "resetting env. episode reward total was 120.000000. running mean: 217.911000\n",
      "[-0.12983571 -0.67973057  0.18065035  0.04119368 -0.15596421 -0.4068718\n",
      "  0.96747509  0.27061028 -0.84796414 -0.50629873 -0.20618205  0.52924054\n",
      " -0.22761757 -0.13798298 -0.00957881 -0.06832917 -0.19058326 -0.26986107\n",
      "  0.02436244  0.27506257  0.06959853 -0.22588664 -0.53179729  0.25059303\n",
      "  0.37783561 -0.18231175 -0.68949402  0.54546612  0.06518226 -0.88654775\n",
      " -0.24712389 -0.2434686   0.55725445  0.38700768  0.46743208  0.05058317\n",
      "  0.5169702  -0.5144325  -0.13437692  0.18886297 -0.20328167  1.36986982\n",
      " -0.11902646 -0.20595888 -0.85088256 -1.10053674 -0.19717739 -0.34535297\n",
      " -0.82017448 -0.20889297  0.29428541 -0.4979566  -0.42174743  0.16020466\n",
      "  0.39654796 -0.28517774  0.15811032 -0.06069071 -1.54188754 -0.40550071\n",
      "  0.13940862 -0.47427648  0.11258485 -0.08571518 -0.31840263  0.35596299\n",
      " -0.45861155 -0.81894091  0.20886745 -0.54861999 -0.54339658 -0.88174316\n",
      " -0.24662412 -0.6568161  -0.28447809 -0.41326884 -0.55191803  0.12307364\n",
      " -0.16319826 -1.03689562 -0.44783156 -0.96796431 -0.0803223  -0.37188793\n",
      " -0.1015313  -0.10856885 -0.05924053  0.31845496 -0.40863879  0.26285225\n",
      " -0.63448657 -1.28890312 -0.08220101 -1.49652356  0.3011036  -0.04794396\n",
      " -0.32750109 -0.72652066  0.13547369 -0.30095782  0.19200952  0.24983875\n",
      " -0.68293686  0.51669787 -0.77394494 -0.27804353 -0.27976608  0.88616604\n",
      " -0.22076256 -0.13010401 -0.0087444  -0.02813347 -0.01144361 -0.3593786\n",
      " -0.03195599  0.01781046 -0.28710445  0.39954219 -0.45707788 -0.09199603\n",
      "  0.02496954 -0.13948007 -0.16270214  0.23067216 -0.11951568  0.42441399\n",
      " -0.69573702 -0.85841092  0.38784928 -0.36759097 -0.23172652  0.62620766\n",
      "  0.19270874 -0.26183829 -0.64678418  0.50662416 -0.22036142  0.94214083\n",
      " -0.40562381 -0.01381149  0.0770762  -0.1725462  -0.21402062  0.75913403\n",
      "  0.08046516 -0.06142191 -0.02960275 -0.13403816 -0.62817743  0.38015513\n",
      "  0.86104578 -0.25420087  0.14228351  0.04437414 -0.82377697  0.2061053\n",
      " -1.29562272  0.48216738  0.71099096 -0.07673967 -0.41929548  0.1710255\n",
      "  0.0103471   0.5694364  -0.6631349  -0.03700461 -0.28237705  0.37558986\n",
      " -0.60870203  0.29143305 -0.16670347  0.86480789  0.21824997  0.1519878\n",
      "  0.10349964  0.09629906  0.18904262  0.23576391  0.3065482   0.16513607\n",
      " -0.87749552 -0.05413662 -0.68404521 -0.22595251 -1.08448876  0.08186073\n",
      "  0.46756858  1.05681046  0.12454172 -0.27964958 -1.03247405  0.40144561\n",
      "  0.54880456  0.16348875 -0.80147776 -0.48101262 -0.65588442  0.17652482\n",
      " -0.63250159 -0.67515185]\n",
      "resetting env. episode reward total was 190.000000. running mean: 217.631890\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b44bd3c0ef1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepisode_number\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'save.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mreward_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reset env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mprev_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ezradavis/anaconda3/envs/py27/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ezradavis/anaconda3/envs/py27/lib/python2.7/site-packages/gym/envs/atari/atari_env.pyc\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# return: (states, observations)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ezradavis/anaconda3/envs/py27/lib/python2.7/site-packages/atari_py/ale_python_interface.pyc\u001b[0m in \u001b[0;36mreset_game\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetLegalActionSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" Trains an agent with (stochastic) Policy Gradients on PacMan. Uses OpenAI Gym. \"\"\"\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import gym\n",
    "\n",
    "# hyperparameters\n",
    "H = 200 # number of hidden layer neurons\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False# resume from previous checkpoint?\n",
    "render = False # whether to render the game. You should turn this off to speed up the program.\n",
    "\n",
    "# model initialization\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "if resume:\n",
    "    model = pickle.load(open('save.p', 'rb'))\n",
    "else:\n",
    "    model = {}\n",
    "    # W1.shape = (hidden_neurons, inputs)\n",
    "    model['W_hidden'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
    "    # output_neurons.shape = (hidden_neurons)\n",
    "    model['W_up'] = np.random.randn(H) / np.sqrt(H)\n",
    "    model['W_down'] = np.random.randn(H) / np.sqrt(H)\n",
    "    model['W_left'] = np.random.randn(H) / np.sqrt(H)\n",
    "    model['W_right'] = np.random.randn(H) / np.sqrt(H)\n",
    "    \n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.iteritems() } # update buffers that add up gradients over a batch\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.iteritems() } # rmsprop memory\n",
    "\n",
    "def sigmoid(x): \n",
    "    return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    #crop the screen\n",
    "    I = np.reshape(I[0:160], (160,160,3))\n",
    "    I = I[:,:,1]\n",
    "    I[I==111]=0\n",
    "    I[I==28]=1\n",
    "    I[I>1]=2\n",
    "    I=I[::2,::2]\n",
    "    return I.astype(np.float).ravel()\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "def policy_forward(x):\n",
    "    h = np.dot(model['W_hidden'], x)\n",
    "    h[h<0] = 0 # ReLU nonlinearity\n",
    "    p_up = sigmoid(np.dot(model['W_up'], h))\n",
    "    p_down = sigmoid(np.dot(model['W_down'], h))\n",
    "    p_left = sigmoid(np.dot(model['W_left'], h))\n",
    "    p_right = sigmoid(np.dot(model['W_right'], h))\n",
    "    total_p = p_up + p_down + p_left + p_right\n",
    "    p_up /= total_p\n",
    "    p_down /= total_p\n",
    "    p_left /= total_p\n",
    "    p_right /= total_p\n",
    "    return [p_up, p_down, p_left, p_right], h # return probability of taking action 2, and hidden state\n",
    "\n",
    "def probabilities_to_action(probabilities):\n",
    "    random = np.random.uniform()\n",
    "    if random < probabilities[0]:\n",
    "        return up\n",
    "    elif random < sum(probabilities[0:2]):\n",
    "        return down\n",
    "    elif random < sum(probabilities[0:3]):\n",
    "        return left\n",
    "    else: # random < sum(probabilities[0:4]):\n",
    "        return right\n",
    "\n",
    "def policy_backward(eph, epdlogp):\n",
    "    \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "    dW_up = np.dot(eph.T, epdlogp[:,0]).ravel() # ravel flaten the matrix into 1-d vector\n",
    "    dW_down = np.dot(eph.T, epdlogp[:,1]).ravel() # ravel flaten the matrix into 1-d vector\n",
    "    dW_left = np.dot(eph.T, epdlogp[:,2]).ravel() # ravel flaten the matrix into 1-d vector\n",
    "    dW_right = np.dot(eph.T, epdlogp[:,3]).ravel() # ravel flaten the matrix into 1-d vector\n",
    "    print dW_up\n",
    "    dh_up = np.outer(epdlogp[:, 0], model['W_up'])\n",
    "    dh_up[eph <= 0] = 0 # backpro prelu\n",
    "    dW_hidden = np.dot(dh_up.T, epx)\n",
    "    dh_down = np.outer(epdlogp[:, 0], model['W_down'])\n",
    "    dh_down[eph <= 0] = 0 # backpro prelu\n",
    "    dW_hidden += np.dot(dh_down.T, epx)\n",
    "    dh_left = np.outer(epdlogp[:, 0], model['W_left'])\n",
    "    dh_left[eph <= 0] = 0 # backpro prelu\n",
    "    dW_hidden = np.dot(dh_left.T, epx)\n",
    "    dh_right = np.outer(epdlogp[:, 0], model['W_right'])\n",
    "    dh_right[eph <= 0] = 0 # backpro prelu\n",
    "    dW_hidden += np.dot(dh_right.T, epx)\n",
    "\n",
    "    return {'W_hidden':dW_hidden, 'W_up':dW_up, 'W_down': dW_down, 'W_left': dW_left, 'W_right': dW_right}\n",
    "\n",
    "env = gym.make(\"MsPacman-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "\n",
    "# xs are board states, hs are hidden neurons over time,\n",
    "# dlogps = [ [label_up, label_down...], ...]\n",
    "# drs = [reward, ...]\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "while True:\n",
    "    if render: env.render()\n",
    "\n",
    "    # preprocess the observation, set input to network to be difference image\n",
    "    cur_x = prepro(observation)\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "    prev_x = cur_x\n",
    "\n",
    "    # forward the policy network and sample an action from the returned probability\n",
    "    probabilities, h = policy_forward(x)\n",
    "    \n",
    "    action = probabilities_to_action(probabilities)\n",
    "\n",
    "    # record various intermediates (needed later for backprop)\n",
    "    xs.append(x) # observation\n",
    "    hs.append(h) # hidden state\n",
    "    labels = np.array([action == up, action == down, action == left, action == right]) - np.array(probabilities)\n",
    "    dlogps.append(labels) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
    "\n",
    "    # step the environment and get new measurements\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "    #print \"reward:%f\" % reward\n",
    "\n",
    "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "    if done: # an episode finished\n",
    "        episode_number += 1\n",
    "\n",
    "        # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "        epx = np.vstack(xs)\n",
    "        eph = np.vstack(hs)\n",
    "        epdlogp = np.vstack(dlogps)\n",
    "        epr = np.vstack(drs)\n",
    "        xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
    "\n",
    "        # compute the discounted reward backwards through time\n",
    "        discounted_epr = discount_rewards(epr)\n",
    "        # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "        discounted_epr -= np.mean(discounted_epr)\n",
    "        discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "        epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
    "        grad = policy_backward(eph, epdlogp)\n",
    "        for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
    "\n",
    "        # perform rmsprop parameter update every batch_size episodes\n",
    "        if episode_number % batch_size == 0:\n",
    "            for k,v in model.iteritems():\n",
    "                g = grad_buffer[k] # gradient\n",
    "                rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "                model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "                grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
    "\n",
    "        # boring book-keeping\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        print 'resetting env. episode reward total was %f. running mean: %f' % (reward_sum, running_reward)\n",
    "        if episode_number % 100 == 0: pickle.dump(model, open('save.p', 'wb'))\n",
    "        reward_sum = 0\n",
    "        observation = env.reset() # reset env\n",
    "        prev_x = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we show two examples\n",
    "### Example 1 (random agent)\n",
    "The following agent can move the PacMan in all directions. But it's random. So it only achieved goal (a): move the paceman in all directions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-28 09:04:27,125] Making new env: MsPacman-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward:220.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "num_episodes = 1 # how many episodes to play\n",
    "render = True # whether to render the game. You should turn this off to speed up the program.\n",
    "\n",
    "class myAgent(object):\n",
    "  def __init__(self,env):\n",
    "    self.env = env\n",
    "    pass      \n",
    "  def pick_next_action(self, observation, reward):\n",
    "    \"\"\" observation is the current screen image. reward is the current reward in the time step \"\"\"\n",
    "    best_action = self.env.action_space.sample() # RANDOMLY pick an action for the next move. \n",
    "    return best_action\n",
    "\n",
    "        \n",
    "env = gym.make('MsPacman-v0') # create the game envirement\n",
    "agent = myAgent(env)# create an agent\n",
    "\n",
    "# let's play some episodes of the game\n",
    "for _ in range(num_episodes): \n",
    "    observation = env.reset() # initialize the game\n",
    "    episode_reward=0 # the sum of rewards in an episode\n",
    "    action = env.action_space.sample() # RANDOMLY pick an action for the next move\n",
    "    observation, reward, done, info = env.step(action) #execute the action and get the reward and next observation\n",
    "    while not done: \n",
    "        if render: \n",
    "            env.render() # render the game\n",
    "        action = agent.pick_next_action(observation,reward)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        episode_reward += reward #adding up the reward in the episode\n",
    "        if done: # the episode is done\n",
    "            print(\"Episode reward:{}\".format(episode_reward))\n",
    "            episode_reward=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2 (left-right agent)\n",
    "The following agent trains a neural network to control the PacMan. But it can only move left and right. \n",
    "So it only achieved goals (b) and (c). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-28 09:04:41,706] Making new env: MsPacman-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode reward total was 80.000000. running mean: 80.000000\n",
      "resetting env. episode reward total was 110.000000. running mean: 80.300000\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Trains an agent with (stochastic) Policy Gradients on PacMan. Uses OpenAI Gym. \"\"\"\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import gym\n",
    "\n",
    "# hyperparameters\n",
    "H = 200 # number of hidden layer neurons\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False# resume from previous checkpoint?\n",
    "render = True # whether to render the game. You should turn this off to speed up the program.\n",
    "\n",
    "# model initialization\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "if resume:\n",
    "  model = pickle.load(open('save.p', 'rb'))\n",
    "else:\n",
    "  model = {}\n",
    "  model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
    "  model[''] = np.random.randn(H) / np.sqrt(H)\n",
    "  \n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.iteritems() } # update buffers that add up gradients over a batch\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.iteritems() } # rmsprop memory\n",
    "\n",
    "def sigmoid(x): \n",
    "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "def prepro(I):\n",
    "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "  #crop the screen\n",
    "  I = np.reshape(I[0:160], (160,160,3))\n",
    "  I = I[:,:,1]\n",
    "  I[I==111]=0\n",
    "  I[I==28]=1\n",
    "  I[I>1]=2\n",
    "  I=I[::2,::2]\n",
    "  return I.astype(np.float).ravel()\n",
    "\n",
    "def discount_rewards(r):\n",
    "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  for t in reversed(xrange(0, r.size)):\n",
    "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "    running_add = running_add * gamma + r[t]\n",
    "    discounted_r[t] = running_add\n",
    "  return discounted_r\n",
    "\n",
    "def policy_forward(x):\n",
    "  h = np.dot(model['W1'], x)\n",
    "  h[h<0] = 0 # ReLU nonlinearity\n",
    "  logp = np.dot(model[''], h)\n",
    "  p = sigmoid(logp)\n",
    "  return p, h # return probability of taking action 2, and hidden state\n",
    "\n",
    "def policy_backward(eph, epdlogp):\n",
    "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "  d = np.dot(eph.T, epdlogp).ravel() # ravel flaten the matrix into 1-d vector\n",
    "  dh = np.outer(epdlogp, model[''])\n",
    "  dh[eph <= 0] = 0 # backpro prelu\n",
    "  dW1 = np.dot(dh.T, epx)\n",
    "  return {'W1':dW1, '':d}\n",
    "\n",
    "env = gym.make(\"MsPacman-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "while True:\n",
    "  if render: env.render()\n",
    "\n",
    "  # preprocess the observation, set input to network to be difference image\n",
    "  cur_x = prepro(observation)\n",
    "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "  prev_x = cur_x\n",
    "\n",
    "  # forward the policy network and sample an action from the returned probability\n",
    "  aprob, h = policy_forward(x)\n",
    "  action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
    "\n",
    "  # record various intermediates (needed later for backprop)\n",
    "  xs.append(x) # observation\n",
    "  hs.append(h) # hidden state\n",
    "  y = 1 if action == 2 else 0 # a \"fake label\"\n",
    "  dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
    "\n",
    "  # step the environment and get new measurements\n",
    "  observation, reward, done, info = env.step(action)\n",
    "  reward_sum += reward\n",
    "  #print \"reward:%f\" % reward\n",
    "\n",
    "  drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "  if done: # an episode finished\n",
    "    episode_number += 1\n",
    "\n",
    "    # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "    epx = np.vstack(xs)\n",
    "    eph = np.vstack(hs)\n",
    "    epdlogp = np.vstack(dlogps)\n",
    "    epr = np.vstack(drs)\n",
    "    xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
    "\n",
    "    # compute the discounted reward backwards through time\n",
    "    discounted_epr = discount_rewards(epr)\n",
    "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "    discounted_epr -= np.mean(discounted_epr)\n",
    "    discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "    epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
    "    grad = policy_backward(eph, epdlogp)\n",
    "    for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
    "\n",
    "    # perform rmsprop parameter update every batch_size episodes\n",
    "    if episode_number % batch_size == 0:\n",
    "      for k,v in model.iteritems():\n",
    "        g = grad_buffer[k] # gradient\n",
    "        rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "        model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "        grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
    "\n",
    "    # boring book-keeping\n",
    "    running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "    print 'resetting env. episode reward total was %f. running mean: %f' % (reward_sum, running_reward)\n",
    "    if episode_number % 100 == 0: pickle.dump(model, open('save.p', 'wb'))\n",
    "    reward_sum = 0\n",
    "    observation = env.reset() # reset env\n",
    "    prev_x = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example code for  processing of the screen image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "env=gym.make('MsPacman-v0')\n",
    "\n",
    "observation = env.reset()\n",
    "observation, reward, done, info = env.step(1)\n",
    "\n",
    "# plot the current screen\n",
    "plt.figure(1)\n",
    "plt.imshow(np.reshape(observation, (210,160,3) ))\n",
    "\n",
    "#crop the screen\n",
    "x = np.reshape(observation[0:171], (171,160,3) )\n",
    "plt.figure(2)\n",
    "plt.imshow(x)\n",
    "\n",
    "\n",
    "# remove background\n",
    "x = x[:,:,1]\n",
    "x[x==111]=0\n",
    "x[x==28]=1\n",
    "x[x>1]=2\n",
    "# downsample\n",
    "x=x[::2,::2]\n",
    "plt.figure(3)\n",
    "plt.imshow(x,cmap='Greys')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-----------------\n",
    "# Done\n",
    "\n",
    "All set! \n",
    "\n",
    "** What do you need to submit?**\n",
    "\n",
    "* **Notebook File**: Save this IPython notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"ipython notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
    "\n",
    "* **Python Code File**: filename: mr_matrix_multiplication.py\n",
    "* **Output File**: results for matrix C. Filename: output\n",
    "\n",
    "\n",
    "** How to submit: **\n",
    "  Please submit your files through myWPI, in the Assignment \"Homework 4\"."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
