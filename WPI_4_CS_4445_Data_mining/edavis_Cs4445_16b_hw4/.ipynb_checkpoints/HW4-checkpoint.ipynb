{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 : MapReduce and Deep Learning (100 points)\n",
    "\n",
    "*------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** NOTE **\n",
    "* Please don't forget to save the notebook frequently when working in IPython Notebook, otherwise the changes you made can be lost.\n",
    "\n",
    "*----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required reading:\n",
    "* [Reinforcement learning](http://karpathy.github.io/2016/05/31/rl/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 (50 points): Using MapReduce framework to implement Matrix Multiplication\n",
    "In this question, you need to use your editor to work on your code, instead of iPython notebook.\n",
    "In the folder, we have a file **input_matrices.data** for the input matrices A and B. \n",
    "The goal is to create a file, named **mr_matrix_multiplication.py**, to compute the matrix C = A X B.\n",
    "In this file, you should use the MapReduce framework to solve the problem, by implementing a *map* function and *reduce* function.\n",
    "\n",
    "Please run your code through the terminal and save the output results into a file named **output**.\n",
    "For example, in linux/mac, you could use the following command in the terminal:\n",
    "\n",
    " *cat input_matrices.data | python mr_matrix_multiplication.py > output*\n",
    " \n",
    " Note in this question, you cannot use any matrix multiplication function in existing packages, such as those in numpy and scipy. The output file should use a format that is similar to the format of the file *input_matrices.data*. Each line of the file represents one element of Matrix C. \n",
    " For example, any of the following formats would be good.\n",
    "* C, 1, 1, 132 \n",
    "* \"C\", 1, 1, 132 \n",
    "* \"C\" (1, 1, 132)\n",
    "* _ (\"C\", 1, 1, 132)\n",
    "\n",
    "Here 132 is just a made-up number, which may not be the correct answer for C[1,1].\n",
    "We assume the indices of the matrix elements start from 1, instead of 0. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (50 points): Reinforcement Learning for Pacman\n",
    "We first introduce an example of how to use the gym package from OpenAI to design an agent for Pacman Game.\n",
    "In the following cell, we implemented a simple agent, which randomly picks the next action without looking at the screen image (i.e., the *observation*) or the reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the following code for **myAgent** to design a better agent, which takes the observation and reward as the input and picks the best action as the next move.\n",
    "The agent should be able to improve itself after playing more games.\n",
    "\n",
    "***The goals***: Implement an agent using neural networks that can achieve all the following goals:\n",
    "* (a) move the PacMan in all directions.  (10 points)\n",
    "* (b) using neural network to decide what is the best next move. (20 points)\n",
    "* (c) after playing each episode of the game, the agent should be able to improve itself using the experience. (20 points)\n",
    "\n",
    "Action Code:\n",
    "* 1 - UP\n",
    "* 2 - RIGHT\n",
    "* 3 - LEFT\n",
    "* 4 - DOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "up = 1\n",
    "right = 2\n",
    "left = 3\n",
    "down = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-28 10:00:10,605] Making new env: MsPacman-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode reward total was 190.000000. running mean: 190.000000\n",
      "resetting env. episode reward total was 190.000000. running mean: 190.000000\n",
      "resetting env. episode reward total was 230.000000. running mean: 190.400000\n",
      "resetting env. episode reward total was 310.000000. running mean: 191.596000\n",
      "resetting env. episode reward total was 190.000000. running mean: 191.580040\n",
      "resetting env. episode reward total was 270.000000. running mean: 192.364240\n",
      "resetting env. episode reward total was 280.000000. running mean: 193.240597\n",
      "resetting env. episode reward total was 360.000000. running mean: 194.908191\n",
      "resetting env. episode reward total was 340.000000. running mean: 196.359109\n",
      "resetting env. episode reward total was 130.000000. running mean: 195.695518\n",
      "resetting env. episode reward total was 200.000000. running mean: 195.738563\n",
      "resetting env. episode reward total was 340.000000. running mean: 197.181177\n",
      "resetting env. episode reward total was 160.000000. running mean: 196.809366\n",
      "resetting env. episode reward total was 180.000000. running mean: 196.641272\n",
      "resetting env. episode reward total was 320.000000. running mean: 197.874859\n",
      "resetting env. episode reward total was 160.000000. running mean: 197.496111\n",
      "resetting env. episode reward total was 510.000000. running mean: 200.621150\n",
      "resetting env. episode reward total was 190.000000. running mean: 200.514938\n",
      "resetting env. episode reward total was 110.000000. running mean: 199.609789\n",
      "resetting env. episode reward total was 220.000000. running mean: 199.813691\n",
      "resetting env. episode reward total was 210.000000. running mean: 199.915554\n",
      "resetting env. episode reward total was 220.000000. running mean: 200.116398\n",
      "resetting env. episode reward total was 120.000000. running mean: 199.315234\n",
      "resetting env. episode reward total was 150.000000. running mean: 198.822082\n",
      "resetting env. episode reward total was 260.000000. running mean: 199.433861\n",
      "resetting env. episode reward total was 300.000000. running mean: 200.439523\n",
      "resetting env. episode reward total was 240.000000. running mean: 200.835127\n",
      "resetting env. episode reward total was 170.000000. running mean: 200.526776\n",
      "resetting env. episode reward total was 200.000000. running mean: 200.521508\n",
      "resetting env. episode reward total was 150.000000. running mean: 200.016293\n",
      "resetting env. episode reward total was 240.000000. running mean: 200.416130\n",
      "resetting env. episode reward total was 200.000000. running mean: 200.411969\n",
      "resetting env. episode reward total was 90.000000. running mean: 199.307849\n",
      "resetting env. episode reward total was 130.000000. running mean: 198.614771\n",
      "resetting env. episode reward total was 170.000000. running mean: 198.328623\n",
      "resetting env. episode reward total was 170.000000. running mean: 198.045337\n",
      "resetting env. episode reward total was 160.000000. running mean: 197.664884\n",
      "resetting env. episode reward total was 160.000000. running mean: 197.288235\n",
      "resetting env. episode reward total was 250.000000. running mean: 197.815352\n",
      "resetting env. episode reward total was 180.000000. running mean: 197.637199\n",
      "resetting env. episode reward total was 170.000000. running mean: 197.360827\n",
      "resetting env. episode reward total was 180.000000. running mean: 197.187219\n",
      "resetting env. episode reward total was 240.000000. running mean: 197.615346\n",
      "resetting env. episode reward total was 200.000000. running mean: 197.639193\n",
      "resetting env. episode reward total was 370.000000. running mean: 199.362801\n",
      "resetting env. episode reward total was 270.000000. running mean: 200.069173\n",
      "resetting env. episode reward total was 280.000000. running mean: 200.868481\n",
      "resetting env. episode reward total was 290.000000. running mean: 201.759796\n",
      "resetting env. episode reward total was 220.000000. running mean: 201.942198\n",
      "resetting env. episode reward total was 140.000000. running mean: 201.322776\n",
      "resetting env. episode reward total was 150.000000. running mean: 200.809549\n",
      "resetting env. episode reward total was 100.000000. running mean: 199.801453\n",
      "resetting env. episode reward total was 180.000000. running mean: 199.603439\n",
      "resetting env. episode reward total was 220.000000. running mean: 199.807404\n",
      "resetting env. episode reward total was 160.000000. running mean: 199.409330\n",
      "resetting env. episode reward total was 570.000000. running mean: 203.115237\n",
      "resetting env. episode reward total was 200.000000. running mean: 203.084085\n",
      "resetting env. episode reward total was 170.000000. running mean: 202.753244\n",
      "resetting env. episode reward total was 200.000000. running mean: 202.725711\n",
      "resetting env. episode reward total was 180.000000. running mean: 202.498454\n",
      "resetting env. episode reward total was 180.000000. running mean: 202.273470\n",
      "resetting env. episode reward total was 160.000000. running mean: 201.850735\n",
      "resetting env. episode reward total was 180.000000. running mean: 201.632228\n",
      "resetting env. episode reward total was 220.000000. running mean: 201.815905\n",
      "resetting env. episode reward total was 280.000000. running mean: 202.597746\n",
      "resetting env. episode reward total was 280.000000. running mean: 203.371769\n",
      "resetting env. episode reward total was 230.000000. running mean: 203.638051\n",
      "resetting env. episode reward total was 170.000000. running mean: 203.301671\n",
      "resetting env. episode reward total was 180.000000. running mean: 203.068654\n",
      "resetting env. episode reward total was 210.000000. running mean: 203.137967\n",
      "resetting env. episode reward total was 1070.000000. running mean: 211.806588\n",
      "resetting env. episode reward total was 160.000000. running mean: 211.288522\n",
      "resetting env. episode reward total was 260.000000. running mean: 211.775637\n",
      "resetting env. episode reward total was 120.000000. running mean: 210.857880\n",
      "resetting env. episode reward total was 550.000000. running mean: 214.249301\n",
      "resetting env. episode reward total was 250.000000. running mean: 214.606808\n",
      "resetting env. episode reward total was 160.000000. running mean: 214.060740\n",
      "resetting env. episode reward total was 270.000000. running mean: 214.620133\n",
      "resetting env. episode reward total was 180.000000. running mean: 214.273932\n",
      "resetting env. episode reward total was 190.000000. running mean: 214.031192\n",
      "resetting env. episode reward total was 170.000000. running mean: 213.590880\n",
      "resetting env. episode reward total was 190.000000. running mean: 213.354972\n",
      "resetting env. episode reward total was 220.000000. running mean: 213.421422\n",
      "resetting env. episode reward total was 330.000000. running mean: 214.587208\n",
      "resetting env. episode reward total was 90.000000. running mean: 213.341336\n",
      "resetting env. episode reward total was 150.000000. running mean: 212.707922\n",
      "resetting env. episode reward total was 150.000000. running mean: 212.080843\n",
      "resetting env. episode reward total was 160.000000. running mean: 211.560035\n",
      "resetting env. episode reward total was 250.000000. running mean: 211.944434\n",
      "resetting env. episode reward total was 180.000000. running mean: 211.624990\n",
      "resetting env. episode reward total was 200.000000. running mean: 211.508740\n",
      "resetting env. episode reward total was 380.000000. running mean: 213.193653\n",
      "resetting env. episode reward total was 190.000000. running mean: 212.961716\n",
      "resetting env. episode reward total was 220.000000. running mean: 213.032099\n",
      "resetting env. episode reward total was 250.000000. running mean: 213.401778\n",
      "resetting env. episode reward total was 190.000000. running mean: 213.167760\n",
      "resetting env. episode reward total was 150.000000. running mean: 212.536083\n",
      "resetting env. episode reward total was 150.000000. running mean: 211.910722\n",
      "resetting env. episode reward total was 120.000000. running mean: 210.991614\n",
      "resetting env. episode reward total was 210.000000. running mean: 210.981698\n",
      "resetting env. episode reward total was 200.000000. running mean: 210.871881\n",
      "resetting env. episode reward total was 150.000000. running mean: 210.263163\n",
      "resetting env. episode reward total was 230.000000. running mean: 210.460531\n",
      "resetting env. episode reward total was 80.000000. running mean: 209.155926\n",
      "resetting env. episode reward total was 120.000000. running mean: 208.264366\n",
      "resetting env. episode reward total was 300.000000. running mean: 209.181723\n",
      "resetting env. episode reward total was 170.000000. running mean: 208.789905\n",
      "resetting env. episode reward total was 100.000000. running mean: 207.702006\n",
      "resetting env. episode reward total was 130.000000. running mean: 206.924986\n",
      "resetting env. episode reward total was 190.000000. running mean: 206.755736\n",
      "resetting env. episode reward total was 150.000000. running mean: 206.188179\n",
      "resetting env. episode reward total was 210.000000. running mean: 206.226297\n",
      "resetting env. episode reward total was 190.000000. running mean: 206.064034\n",
      "resetting env. episode reward total was 150.000000. running mean: 205.503394\n",
      "resetting env. episode reward total was 240.000000. running mean: 205.848360\n",
      "resetting env. episode reward total was 200.000000. running mean: 205.789876\n",
      "resetting env. episode reward total was 200.000000. running mean: 205.731978\n",
      "resetting env. episode reward total was 380.000000. running mean: 207.474658\n",
      "resetting env. episode reward total was 140.000000. running mean: 206.799911\n",
      "resetting env. episode reward total was 230.000000. running mean: 207.031912\n",
      "resetting env. episode reward total was 230.000000. running mean: 207.261593\n",
      "resetting env. episode reward total was 490.000000. running mean: 210.088977\n",
      "resetting env. episode reward total was 240.000000. running mean: 210.388087\n",
      "resetting env. episode reward total was 230.000000. running mean: 210.584207\n",
      "resetting env. episode reward total was 210.000000. running mean: 210.578364\n",
      "resetting env. episode reward total was 180.000000. running mean: 210.272581\n",
      "resetting env. episode reward total was 120.000000. running mean: 209.369855\n",
      "resetting env. episode reward total was 260.000000. running mean: 209.876156\n",
      "resetting env. episode reward total was 230.000000. running mean: 210.077395\n",
      "resetting env. episode reward total was 180.000000. running mean: 209.776621\n",
      "resetting env. episode reward total was 70.000000. running mean: 208.378855\n",
      "resetting env. episode reward total was 180.000000. running mean: 208.095066\n",
      "resetting env. episode reward total was 260.000000. running mean: 208.614116\n",
      "resetting env. episode reward total was 150.000000. running mean: 208.027974\n",
      "resetting env. episode reward total was 80.000000. running mean: 206.747695\n",
      "resetting env. episode reward total was 170.000000. running mean: 206.380218\n",
      "resetting env. episode reward total was 270.000000. running mean: 207.016415\n",
      "resetting env. episode reward total was 220.000000. running mean: 207.146251\n",
      "resetting env. episode reward total was 240.000000. running mean: 207.474789\n",
      "resetting env. episode reward total was 260.000000. running mean: 208.000041\n",
      "resetting env. episode reward total was 120.000000. running mean: 207.120041\n",
      "resetting env. episode reward total was 230.000000. running mean: 207.348840\n",
      "resetting env. episode reward total was 550.000000. running mean: 210.775352\n",
      "resetting env. episode reward total was 100.000000. running mean: 209.667598\n",
      "resetting env. episode reward total was 120.000000. running mean: 208.770922\n",
      "resetting env. episode reward total was 230.000000. running mean: 208.983213\n",
      "resetting env. episode reward total was 150.000000. running mean: 208.393381\n",
      "resetting env. episode reward total was 220.000000. running mean: 208.509447\n",
      "resetting env. episode reward total was 330.000000. running mean: 209.724353\n",
      "resetting env. episode reward total was 360.000000. running mean: 211.227109\n",
      "resetting env. episode reward total was 390.000000. running mean: 213.014838\n",
      "resetting env. episode reward total was 150.000000. running mean: 212.384690\n",
      "resetting env. episode reward total was 90.000000. running mean: 211.160843\n",
      "resetting env. episode reward total was 160.000000. running mean: 210.649234\n",
      "resetting env. episode reward total was 270.000000. running mean: 211.242742\n",
      "resetting env. episode reward total was 100.000000. running mean: 210.130315\n",
      "resetting env. episode reward total was 240.000000. running mean: 210.429011\n",
      "resetting env. episode reward total was 160.000000. running mean: 209.924721\n",
      "resetting env. episode reward total was 340.000000. running mean: 211.225474\n",
      "resetting env. episode reward total was 270.000000. running mean: 211.813219\n",
      "resetting env. episode reward total was 200.000000. running mean: 211.695087\n",
      "resetting env. episode reward total was 540.000000. running mean: 214.978136\n",
      "resetting env. episode reward total was 130.000000. running mean: 214.128355\n",
      "resetting env. episode reward total was 220.000000. running mean: 214.187071\n",
      "resetting env. episode reward total was 230.000000. running mean: 214.345201\n",
      "resetting env. episode reward total was 270.000000. running mean: 214.901749\n",
      "resetting env. episode reward total was 230.000000. running mean: 215.052731\n",
      "resetting env. episode reward total was 120.000000. running mean: 214.102204\n",
      "resetting env. episode reward total was 140.000000. running mean: 213.361182\n",
      "resetting env. episode reward total was 160.000000. running mean: 212.827570\n",
      "resetting env. episode reward total was 140.000000. running mean: 212.099294\n",
      "resetting env. episode reward total was 210.000000. running mean: 212.078301\n",
      "resetting env. episode reward total was 190.000000. running mean: 211.857518\n",
      "resetting env. episode reward total was 180.000000. running mean: 211.538943\n",
      "resetting env. episode reward total was 130.000000. running mean: 210.723554\n",
      "resetting env. episode reward total was 170.000000. running mean: 210.316318\n",
      "resetting env. episode reward total was 130.000000. running mean: 209.513155\n",
      "resetting env. episode reward total was 200.000000. running mean: 209.418023\n",
      "resetting env. episode reward total was 190.000000. running mean: 209.223843\n",
      "resetting env. episode reward total was 570.000000. running mean: 212.831605\n",
      "resetting env. episode reward total was 210.000000. running mean: 212.803289\n",
      "resetting env. episode reward total was 110.000000. running mean: 211.775256\n",
      "resetting env. episode reward total was 210.000000. running mean: 211.757503\n",
      "resetting env. episode reward total was 150.000000. running mean: 211.139928\n",
      "resetting env. episode reward total was 230.000000. running mean: 211.328529\n",
      "resetting env. episode reward total was 250.000000. running mean: 211.715244\n",
      "resetting env. episode reward total was 110.000000. running mean: 210.698091\n",
      "resetting env. episode reward total was 230.000000. running mean: 210.891110\n",
      "resetting env. episode reward total was 120.000000. running mean: 209.982199\n",
      "resetting env. episode reward total was 170.000000. running mean: 209.582377\n",
      "resetting env. episode reward total was 180.000000. running mean: 209.286553\n",
      "resetting env. episode reward total was 150.000000. running mean: 208.693688\n",
      "resetting env. episode reward total was 200.000000. running mean: 208.606751\n",
      "resetting env. episode reward total was 200.000000. running mean: 208.520684\n",
      "resetting env. episode reward total was 250.000000. running mean: 208.935477\n",
      "resetting env. episode reward total was 180.000000. running mean: 208.646122\n",
      "resetting env. episode reward total was 200.000000. running mean: 208.559661\n",
      "resetting env. episode reward total was 230.000000. running mean: 208.774064\n",
      "resetting env. episode reward total was 210.000000. running mean: 208.786323\n",
      "resetting env. episode reward total was 180.000000. running mean: 208.498460\n",
      "resetting env. episode reward total was 90.000000. running mean: 207.313476\n",
      "resetting env. episode reward total was 560.000000. running mean: 210.840341\n",
      "resetting env. episode reward total was 150.000000. running mean: 210.231937\n",
      "resetting env. episode reward total was 210.000000. running mean: 210.229618\n",
      "resetting env. episode reward total was 150.000000. running mean: 209.627322\n",
      "resetting env. episode reward total was 170.000000. running mean: 209.231049\n",
      "resetting env. episode reward total was 250.000000. running mean: 209.638738\n",
      "resetting env. episode reward total was 240.000000. running mean: 209.942351\n",
      "resetting env. episode reward total was 210.000000. running mean: 209.942927\n",
      "resetting env. episode reward total was 210.000000. running mean: 209.943498\n",
      "resetting env. episode reward total was 170.000000. running mean: 209.544063\n",
      "resetting env. episode reward total was 180.000000. running mean: 209.248622\n",
      "resetting env. episode reward total was 200.000000. running mean: 209.156136\n",
      "resetting env. episode reward total was 290.000000. running mean: 209.964575\n",
      "resetting env. episode reward total was 190.000000. running mean: 209.764929\n",
      "resetting env. episode reward total was 180.000000. running mean: 209.467280\n",
      "resetting env. episode reward total was 350.000000. running mean: 210.872607\n",
      "resetting env. episode reward total was 90.000000. running mean: 209.663881\n",
      "resetting env. episode reward total was 240.000000. running mean: 209.967242\n",
      "resetting env. episode reward total was 220.000000. running mean: 210.067570\n",
      "resetting env. episode reward total was 180.000000. running mean: 209.766894\n",
      "resetting env. episode reward total was 150.000000. running mean: 209.169225\n",
      "resetting env. episode reward total was 210.000000. running mean: 209.177533\n",
      "resetting env. episode reward total was 120.000000. running mean: 208.285757\n",
      "resetting env. episode reward total was 170.000000. running mean: 207.902900\n",
      "resetting env. episode reward total was 200.000000. running mean: 207.823871\n",
      "resetting env. episode reward total was 260.000000. running mean: 208.345632\n",
      "resetting env. episode reward total was 140.000000. running mean: 207.662176\n",
      "resetting env. episode reward total was 230.000000. running mean: 207.885554\n",
      "resetting env. episode reward total was 100.000000. running mean: 206.806699\n",
      "resetting env. episode reward total was 390.000000. running mean: 208.638632\n",
      "resetting env. episode reward total was 170.000000. running mean: 208.252245\n",
      "resetting env. episode reward total was 280.000000. running mean: 208.969723\n",
      "resetting env. episode reward total was 120.000000. running mean: 208.080026\n",
      "resetting env. episode reward total was 120.000000. running mean: 207.199225\n",
      "resetting env. episode reward total was 170.000000. running mean: 206.827233\n",
      "resetting env. episode reward total was 190.000000. running mean: 206.658961\n",
      "resetting env. episode reward total was 310.000000. running mean: 207.692371\n",
      "resetting env. episode reward total was 100.000000. running mean: 206.615447\n",
      "resetting env. episode reward total was 140.000000. running mean: 205.949293\n",
      "resetting env. episode reward total was 120.000000. running mean: 205.089800\n",
      "resetting env. episode reward total was 150.000000. running mean: 204.538902\n",
      "resetting env. episode reward total was 260.000000. running mean: 205.093513\n",
      "resetting env. episode reward total was 220.000000. running mean: 205.242578\n",
      "resetting env. episode reward total was 240.000000. running mean: 205.590152\n",
      "resetting env. episode reward total was 170.000000. running mean: 205.234251\n",
      "resetting env. episode reward total was 200.000000. running mean: 205.181908\n",
      "resetting env. episode reward total was 230.000000. running mean: 205.430089\n",
      "resetting env. episode reward total was 250.000000. running mean: 205.875788\n",
      "resetting env. episode reward total was 110.000000. running mean: 204.917030\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-46e4651e7b3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;31m# step the environment and get new measurements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0mreward_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;31m#print \"reward:%f\" % reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ezradavis/anaconda3/envs/py27/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \"\"\"\n\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ezradavis/anaconda3/envs/py27/lib/python2.7/site-packages/gym/envs/atari/atari_env.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ezradavis/anaconda3/envs/py27/lib/python2.7/site-packages/gym/envs/atari/atari_env.pyc\u001b[0m in \u001b[0;36m_get_obs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ezradavis/anaconda3/envs/py27/lib/python2.7/site-packages/gym/envs/atari/atari_env.pyc\u001b[0m in \u001b[0;36m_get_image\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# says rgb but actually bgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" Trains an agent with (stochastic) Policy Gradients on PacMan. Uses OpenAI Gym. \"\"\"\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import gym\n",
    "\n",
    "# hyperparameters\n",
    "H = 200 # number of hidden layer neurons\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False# resume from previous checkpoint?\n",
    "render = False # whether to render the game. You should turn this off to speed up the program.\n",
    "\n",
    "# model initialization\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "if resume:\n",
    "    model = pickle.load(open('save.p', 'rb'))\n",
    "else:\n",
    "    model = {}\n",
    "    # W1.shape = (hidden_neurons, inputs)\n",
    "    model['W_hidden'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
    "    # output_neurons.shape = (hidden_neurons)\n",
    "    model['W_up'] = np.random.randn(H) / np.sqrt(H)\n",
    "    model['W_down'] = np.random.randn(H) / np.sqrt(H)\n",
    "    model['W_left'] = np.random.randn(H) / np.sqrt(H)\n",
    "    model['W_right'] = np.random.randn(H) / np.sqrt(H)\n",
    "    \n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.iteritems() } # update buffers that add up gradients over a batch\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.iteritems() } # rmsprop memory\n",
    "\n",
    "def sigmoid(x): \n",
    "    return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    #crop the screen\n",
    "    I = np.reshape(I[0:160], (160,160,3))\n",
    "    I = I[:,:,1]\n",
    "    I[I==111]=0\n",
    "    I[I==28]=1\n",
    "    I[I>1]=2\n",
    "    I=I[::2,::2]\n",
    "    return I.astype(np.float).ravel()\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "def policy_forward(x):\n",
    "    h = np.dot(model['W_hidden'], x)\n",
    "    h[h<0] = 0 # ReLU nonlinearity\n",
    "    p_up = sigmoid(np.dot(model['W_up'], h))\n",
    "    p_down = sigmoid(np.dot(model['W_down'], h))\n",
    "    p_left = sigmoid(np.dot(model['W_left'], h))\n",
    "    p_right = sigmoid(np.dot(model['W_right'], h))\n",
    "    total_p = p_up + p_down + p_left + p_right\n",
    "    p_up /= total_p\n",
    "    p_down /= total_p\n",
    "    p_left /= total_p\n",
    "    p_right /= total_p\n",
    "    return [p_up, p_down, p_left, p_right], h # return probability of taking action 2, and hidden state\n",
    "\n",
    "def probabilities_to_action(probabilities):\n",
    "    random = np.random.uniform()\n",
    "    if random < probabilities[0]:\n",
    "        return up\n",
    "    elif random < sum(probabilities[0:2]):\n",
    "        return down\n",
    "    elif random < sum(probabilities[0:3]):\n",
    "        return left\n",
    "    else: # random < sum(probabilities[0:4]):\n",
    "        return right\n",
    "\n",
    "def policy_backward(eph, epdlogp):\n",
    "    \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "    dW_up = np.dot(eph.T, epdlogp[:,0]).ravel() # ravel flaten the matrix into 1-d vector\n",
    "    dW_down = np.dot(eph.T, epdlogp[:,1]).ravel() # ravel flaten the matrix into 1-d vector\n",
    "    dW_left = np.dot(eph.T, epdlogp[:,2]).ravel() # ravel flaten the matrix into 1-d vector\n",
    "    dW_right = np.dot(eph.T, epdlogp[:,3]).ravel() # ravel flaten the matrix into 1-d vector\n",
    "    \n",
    "    dh_up = np.outer(epdlogp[:, 0], model['W_up'])\n",
    "    dh_up[eph <= 0] = 0 # backpro prelu\n",
    "    dW_hidden = np.dot(dh_up.T, epx)\n",
    "    dh_down = np.outer(epdlogp[:, 0], model['W_down'])\n",
    "    dh_down[eph <= 0] = 0 # backpro prelu\n",
    "    dW_hidden += np.dot(dh_down.T, epx)\n",
    "    dh_left = np.outer(epdlogp[:, 0], model['W_left'])\n",
    "    dh_left[eph <= 0] = 0 # backpro prelu\n",
    "    dW_hidden = np.dot(dh_left.T, epx)\n",
    "    dh_right = np.outer(epdlogp[:, 0], model['W_right'])\n",
    "    dh_right[eph <= 0] = 0 # backpro prelu\n",
    "    dW_hidden += np.dot(dh_right.T, epx)\n",
    "\n",
    "    return {'W_hidden':dW_hidden, 'W_up':dW_up, 'W_down': dW_down, 'W_left': dW_left, 'W_right': dW_right}\n",
    "\n",
    "env = gym.make(\"MsPacman-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "\n",
    "# xs are board states, hs are hidden neurons over time,\n",
    "# dlogps = [ [label_up, label_down...], ...]\n",
    "# drs = [reward, ...]\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "while True:\n",
    "    if render: env.render()\n",
    "\n",
    "    # preprocess the observation, set input to network to be difference image\n",
    "    cur_x = prepro(observation)\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "    prev_x = cur_x\n",
    "\n",
    "    # forward the policy network and sample an action from the returned probability\n",
    "    probabilities, h = policy_forward(x)\n",
    "    \n",
    "    action = probabilities_to_action(probabilities)\n",
    "\n",
    "    # record various intermediates (needed later for backprop)\n",
    "    xs.append(x) # observation\n",
    "    hs.append(h) # hidden state\n",
    "    labels = np.array([action == up, action == down, action == left, action == right]) - np.array(probabilities)\n",
    "    dlogps.append(labels) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
    "\n",
    "    # step the environment and get new measurements\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "    #print \"reward:%f\" % reward\n",
    "\n",
    "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "    if done: # an episode finished\n",
    "        episode_number += 1\n",
    "\n",
    "        # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "        epx = np.vstack(xs)\n",
    "        eph = np.vstack(hs)\n",
    "        epdlogp = np.vstack(dlogps)\n",
    "        epr = np.vstack(drs)\n",
    "        xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
    "\n",
    "        # compute the discounted reward backwards through time\n",
    "        discounted_epr = discount_rewards(epr)\n",
    "        # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "        discounted_epr -= np.mean(discounted_epr)\n",
    "        discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "        epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
    "        grad = policy_backward(eph, epdlogp)\n",
    "        for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
    "\n",
    "        # perform rmsprop parameter update every batch_size episodes\n",
    "        if episode_number % batch_size == 0:\n",
    "            for k,v in model.iteritems():\n",
    "                g = grad_buffer[k] # gradient\n",
    "                rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "                model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "                grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
    "\n",
    "        # boring book-keeping\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        print 'resetting env. episode reward total was %f. running mean: %f' % (reward_sum, running_reward)\n",
    "        if episode_number % 100 == 0: pickle.dump(model, open('save.p', 'wb'))\n",
    "        reward_sum = 0\n",
    "        observation = env.reset() # reset env\n",
    "        prev_x = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we show two examples\n",
    "### Example 1 (random agent)\n",
    "The following agent can move the PacMan in all directions. But it's random. So it only achieved goal (a): move the paceman in all directions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-28 09:04:27,125] Making new env: MsPacman-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward:220.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "num_episodes = 1 # how many episodes to play\n",
    "render = True # whether to render the game. You should turn this off to speed up the program.\n",
    "\n",
    "class myAgent(object):\n",
    "  def __init__(self,env):\n",
    "    self.env = env\n",
    "    pass      \n",
    "  def pick_next_action(self, observation, reward):\n",
    "    \"\"\" observation is the current screen image. reward is the current reward in the time step \"\"\"\n",
    "    best_action = self.env.action_space.sample() # RANDOMLY pick an action for the next move. \n",
    "    return best_action\n",
    "\n",
    "        \n",
    "env = gym.make('MsPacman-v0') # create the game envirement\n",
    "agent = myAgent(env)# create an agent\n",
    "\n",
    "# let's play some episodes of the game\n",
    "for _ in range(num_episodes): \n",
    "    observation = env.reset() # initialize the game\n",
    "    episode_reward=0 # the sum of rewards in an episode\n",
    "    action = env.action_space.sample() # RANDOMLY pick an action for the next move\n",
    "    observation, reward, done, info = env.step(action) #execute the action and get the reward and next observation\n",
    "    while not done: \n",
    "        if render: \n",
    "            env.render() # render the game\n",
    "        action = agent.pick_next_action(observation,reward)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        episode_reward += reward #adding up the reward in the episode\n",
    "        if done: # the episode is done\n",
    "            print(\"Episode reward:{}\".format(episode_reward))\n",
    "            episode_reward=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2 (left-right agent)\n",
    "The following agent trains a neural network to control the PacMan. But it can only move left and right. \n",
    "So it only achieved goals (b) and (c). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-28 09:04:41,706] Making new env: MsPacman-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode reward total was 80.000000. running mean: 80.000000\n",
      "resetting env. episode reward total was 110.000000. running mean: 80.300000\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Trains an agent with (stochastic) Policy Gradients on PacMan. Uses OpenAI Gym. \"\"\"\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import gym\n",
    "\n",
    "# hyperparameters\n",
    "H = 200 # number of hidden layer neurons\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False# resume from previous checkpoint?\n",
    "render = True # whether to render the game. You should turn this off to speed up the program.\n",
    "\n",
    "# model initialization\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "if resume:\n",
    "  model = pickle.load(open('save.p', 'rb'))\n",
    "else:\n",
    "  model = {}\n",
    "  model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
    "  model[''] = np.random.randn(H) / np.sqrt(H)\n",
    "  \n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.iteritems() } # update buffers that add up gradients over a batch\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.iteritems() } # rmsprop memory\n",
    "\n",
    "def sigmoid(x): \n",
    "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "def prepro(I):\n",
    "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "  #crop the screen\n",
    "  I = np.reshape(I[0:160], (160,160,3))\n",
    "  I = I[:,:,1]\n",
    "  I[I==111]=0\n",
    "  I[I==28]=1\n",
    "  I[I>1]=2\n",
    "  I=I[::2,::2]\n",
    "  return I.astype(np.float).ravel()\n",
    "\n",
    "def discount_rewards(r):\n",
    "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  for t in reversed(xrange(0, r.size)):\n",
    "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "    running_add = running_add * gamma + r[t]\n",
    "    discounted_r[t] = running_add\n",
    "  return discounted_r\n",
    "\n",
    "def policy_forward(x):\n",
    "  h = np.dot(model['W1'], x)\n",
    "  h[h<0] = 0 # ReLU nonlinearity\n",
    "  logp = np.dot(model[''], h)\n",
    "  p = sigmoid(logp)\n",
    "  return p, h # return probability of taking action 2, and hidden state\n",
    "\n",
    "def policy_backward(eph, epdlogp):\n",
    "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "  d = np.dot(eph.T, epdlogp).ravel() # ravel flaten the matrix into 1-d vector\n",
    "  dh = np.outer(epdlogp, model[''])\n",
    "  dh[eph <= 0] = 0 # backpro prelu\n",
    "  dW1 = np.dot(dh.T, epx)\n",
    "  return {'W1':dW1, '':d}\n",
    "\n",
    "env = gym.make(\"MsPacman-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "while True:\n",
    "  if render: env.render()\n",
    "\n",
    "  # preprocess the observation, set input to network to be difference image\n",
    "  cur_x = prepro(observation)\n",
    "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "  prev_x = cur_x\n",
    "\n",
    "  # forward the policy network and sample an action from the returned probability\n",
    "  aprob, h = policy_forward(x)\n",
    "  action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
    "\n",
    "  # record various intermediates (needed later for backprop)\n",
    "  xs.append(x) # observation\n",
    "  hs.append(h) # hidden state\n",
    "  y = 1 if action == 2 else 0 # a \"fake label\"\n",
    "  dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
    "\n",
    "  # step the environment and get new measurements\n",
    "  observation, reward, done, info = env.step(action)\n",
    "  reward_sum += reward\n",
    "  #print \"reward:%f\" % reward\n",
    "\n",
    "  drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "  if done: # an episode finished\n",
    "    episode_number += 1\n",
    "\n",
    "    # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "    epx = np.vstack(xs)\n",
    "    eph = np.vstack(hs)\n",
    "    epdlogp = np.vstack(dlogps)\n",
    "    epr = np.vstack(drs)\n",
    "    xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
    "\n",
    "    # compute the discounted reward backwards through time\n",
    "    discounted_epr = discount_rewards(epr)\n",
    "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "    discounted_epr -= np.mean(discounted_epr)\n",
    "    discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "    epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
    "    grad = policy_backward(eph, epdlogp)\n",
    "    for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
    "\n",
    "    # perform rmsprop parameter update every batch_size episodes\n",
    "    if episode_number % batch_size == 0:\n",
    "      for k,v in model.iteritems():\n",
    "        g = grad_buffer[k] # gradient\n",
    "        rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "        model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "        grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
    "\n",
    "    # boring book-keeping\n",
    "    running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "    print 'resetting env. episode reward total was %f. running mean: %f' % (reward_sum, running_reward)\n",
    "    if episode_number % 100 == 0: pickle.dump(model, open('save.p', 'wb'))\n",
    "    reward_sum = 0\n",
    "    observation = env.reset() # reset env\n",
    "    prev_x = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example code for  processing of the screen image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "env=gym.make('MsPacman-v0')\n",
    "\n",
    "observation = env.reset()\n",
    "observation, reward, done, info = env.step(1)\n",
    "\n",
    "# plot the current screen\n",
    "plt.figure(1)\n",
    "plt.imshow(np.reshape(observation, (210,160,3) ))\n",
    "\n",
    "#crop the screen\n",
    "x = np.reshape(observation[0:171], (171,160,3) )\n",
    "plt.figure(2)\n",
    "plt.imshow(x)\n",
    "\n",
    "\n",
    "# remove background\n",
    "x = x[:,:,1]\n",
    "x[x==111]=0\n",
    "x[x==28]=1\n",
    "x[x>1]=2\n",
    "# downsample\n",
    "x=x[::2,::2]\n",
    "plt.figure(3)\n",
    "plt.imshow(x,cmap='Greys')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-----------------\n",
    "# Done\n",
    "\n",
    "All set! \n",
    "\n",
    "** What do you need to submit?**\n",
    "\n",
    "* **Notebook File**: Save this IPython notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"ipython notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
    "\n",
    "* **Python Code File**: filename: mr_matrix_multiplication.py\n",
    "* **Output File**: results for matrix C. Filename: output\n",
    "\n",
    "\n",
    "** How to submit: **\n",
    "  Please submit your files through myWPI, in the Assignment \"Homework 4\"."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
