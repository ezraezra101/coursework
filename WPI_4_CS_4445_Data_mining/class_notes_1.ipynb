{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes day one:\n",
    "\n",
    "We're using mywpi, and https://web.cs.wpi.edu/~xkong/courses/cs4445_4803_B16/index.html\n",
    "\n",
    "7 homework assignments (drop lowest 2)\n",
    "7 quizzes (drop lowest 2)\n",
    "(Grade is 50% homework, 50% quizzes)\n",
    "\n",
    "Use python 2\n",
    "\n",
    "\n",
    "Modern GPUs are ~7GFlops (such as the one in my computer)\n",
    "\"the calculated fp64 peak of Titan is 1.5 TFlops\" (it heats up a lot when you do that too)\n",
    "\n",
    "\n",
    "Robots.txt is about webcrawlers.\n",
    "https://www.facebook.com/robots.txt (says: please don't webcrawl without permission)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Notes day 2:\n",
    "\n",
    "We've learned about scatter plots (google public health), histograms, mean, median, mode...\n",
    "We also learned about Flatland: https://www.youtube.com/watch?v=MGv8MMi8QO0\n",
    "\n",
    "### Histograms:\n",
    "Histograms are like:\n",
    "![histogram example](http://matplotlib.org/1.2.1/_images/histogram_demo2.png)\n",
    "and have ennumerative bins - the bars represent a range of values (a \"bin\") on the X axis, not qualitative values.\n",
    "Also, the bars touch each other.\n",
    "\n",
    "\n",
    "### Parallel coordinate plots:\n",
    "When you're comparing categorized multi-dimensional data, you can chart the value vs each of the dimensions.\n",
    "Here's an example with Irises:\n",
    "![](http://upload.wikimedia.org/wikipedia/en/4/4a/ParCorFisherIris.png)\n",
    "\n",
    "\n",
    "#### Star plots (AKA radar plots)\n",
    "Has information about a single datapoint (or several in different colors that allows basic comparison in multidimensional data.\n",
    "\n",
    "Technically, they are the same as parallel coordinate plots except in polar coordinates.\n",
    "\n",
    "![](http://assessment.tki.org.nz/var/tki-assess/storage/images/media/images/obs-star-4/9854-1-eng-NZ/obs-star-4.png)\n",
    "\n",
    "Chernov faces are contain similar information, but use the size of attributes on cartoon faces.\n",
    "\n",
    "#### Power law distributions\n",
    "Some datasets (word frequency, wealth distribution, friends on facebook, hyperlinks) follow the power laws.\n",
    "\n",
    "Also called Zipf's law/distributions.\n",
    "\n",
    "\n",
    "##### Geometric mean:\n",
    "(0.001 * 100 * ...)^(1/n)\n",
    "##### Arithmatic mean:\n",
    "(0.001 + 100 + ...)/n - pays more attention to large values than the geometric mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High dimensional data:\n",
    "\n",
    "### Rule 2 of high dimensional data: All datapoints are on the surface, and not  close the origin of the distribution.\n",
    "If you look at how far points are from the center in a distrubution, in high dimensional data, they tend to not be too close.\n",
    "(E.G. in a high dimensional gaussian (normal) distribution, the average distance from the center (average location) is very much not 0)\n",
    "#### Determining distance between datapoints.\n",
    "Because of the above, the euclidian distance isn't useful (almost all points are far away from each other in at least one dimension) in high dimensional data.\n",
    "\n",
    "Instead, we often use the **cosine distance** $dist(A,B) = cos(\\theta_{between A and B})$\n",
    "\n",
    "### Data quantity\n",
    "As we increase the dimensions, the density of datapoints goes down, which means that it's pretty tricky to get enough samples to be able to predict everything.\n",
    "\n",
    "This is called the **Curse of Dimensionality**.\n",
    "\n",
    "This can be combatted by using angle from the origin (cosine distance) or dimensional reduction or ...\n",
    "\n",
    "### Dimensional reduction\n",
    "\n",
    "Note: Dimensions are Orthogonal, Ordered, and have Fixed direction\n",
    "\n",
    "We want to combine dimensions - turning 2 dimensions into 1, while keeping the highest possible variance (and throwing away the least data).\n",
    "Finding the highest variance directions uses **Principal Component Analysis** or **PCA**.\n",
    "PCA takes in data, and produces a series of dimensions in decreasing variance.\n",
    "Example: http://setosa.io/ev/principal-component-analysis/\n",
    "\n",
    "How it's computed: (guide: https://plot.ly/ipython-notebooks/principal-component-analysis/)\n",
    "\n",
    "1. Normalize the data (make each dimension have mean=0, variance=1)\n",
    "2. Compute Covariance matrix\n",
    "3. Get the Eigenvectors of the Covariance matrix. These are your new dimensions. The bigger the eigenvalue, the more variance you preserve.\n",
    "    Remember: an $A\\cdot \\vec{v} = \\lambda \\vec{v}$ Where $A$ = Covariance matrix, $\\vec{v}$ = eigenvector if it's non-zero. Note: we probably normalize eigenvectors, $\\lambda$ = eigenvalue. There can be many eigenvectors per matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# November 7th\n",
    "\n",
    "### Nearest Neighbor Classifier\n",
    "We have some test data, and and when we get a new sample that we want to predict, we find the closest datum. ([Voronoi diagram](https://en.wikipedia.org/wiki/Voronoi_diagram))\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/5/54/Euclidean_Voronoi_diagram.svg/440px-Euclidean_Voronoi_diagram.svg.png \"hmm\")\n",
    "\n",
    "#### K-nearest neighbor\n",
    "The same as above, but we use the N (or K) nearest points.\n",
    "\n",
    "**Model Selection**: Questions like: How to choose parameter k?\n",
    "\n",
    "\n",
    "### Cross Validation/K-Fold\n",
    "Split your data into 5 groups or so. One of which is the testing data, the other 4 are used for determining accuracy.\n",
    "We can swap out which parts are used for testing/validation, which is very useful for doing model selection.\n",
    "\n",
    "### Support Vector Machine\n",
    "[Wikipedia article](https://en.wikipedia.org/wiki/Support_vector_machine)\n",
    "\n",
    "If we are doing a binary classification problem in N-space, we can divide it into two groups with a line/plane/hyperplane...\n",
    "This can be represented as: $\\vec{w}\\cdot \\vec{x} = c$ where $\\vec{w}$ is a vector and $c$ is a constant.\n",
    "\n",
    "If it's easy to create lines that divide the data in half (without mislabeling), we can find the distance between the line and each point, and try to maximize the average distance. OR: If we give the line thickness, we try and find the thickest line without hitting any other points.\n",
    "\"Thickness\" of line / margin = ($x_+ - x_-)' \\frac{w}{|w|} = \\frac{2}{|w|}$\n",
    "\n",
    "So, we want to find:\n",
    "\n",
    "$min(\\frac{|w|^2}{2})$ \n",
    "with the constraint $y_i(\\vec{w}\\vec{x_i}+b \\geq 1$\n",
    "\n",
    "Use [Lagrange optimization/multiplier](https://en.wikipedia.org/wiki/Lagrange_multiplier)\n",
    "\n",
    "$y_i(\\vec{w}\\cdot\\vec{x_i}+b \\geq 1$ (where $b = -c$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# November 10\n",
    "\n",
    "### SVM (Support Vector Machines) cont.\n",
    "#### Kernel Trick\n",
    "SVMs are very good at determining linear boundaries, but aren't good at non-linear ones.\n",
    "\n",
    "To solve this, we can map each datapoint to a (usually) higher dimensional space with a function that is selected to create a different shaped boundary.\n",
    "E.G. $\\Phi: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3 $ $\\Phi(x_1, x_2) = (x_1, x_2, \\sqrt{x_1^2 + x_2^2})$\n",
    "\n",
    "And then we do $\\vec{w} \\cdot \\Phi(x_1, x_2) \\geq 0 $, if it's true, then it is in the set, otherwise, no.\n",
    "\n",
    "\n",
    "#### Stuffs that I wasn't paying enough attention to:\n",
    "$$min ||w|| + C\\Sigma \\epsilon_i $$\n",
    "$$ s, t ... y_2(w'x_i+b) \\geq 1- \\epsilon_i$$\n",
    "\n",
    "\n",
    "### Map reduce\n",
    "\n",
    "Right, so big companies like Google get too much data, and they need to parallelize.\n",
    "\n",
    "They do this in two (or more) parts:\n",
    "\n",
    "- Map - process an individual input, and send the appropriate outputs to the correct Reduce functions\n",
    "- Reduce - get the result from the output of the data.\n",
    "\n",
    "Map creates a bunch of key value pairs, which gets shuffled off to the appropriate reduce function.\n",
    "\n",
    "\n",
    "#### E.G. Word counting\n",
    "Map:\n",
    "- \"Hello world\" ----> {\"Hello\": 1, \"world\": 1}\n",
    "- \"Hello my name is name\" ----> {\"Hello\": 1, \"my\": 1, \"name\": 2, \"is\": 1}\n",
    "Reduce:\n",
    "- {\"Hello\": [1, 1], \"my\": [1] ....} ----> {\"Hello\": 2, .... }\n",
    "\n",
    "```python\n",
    "# Code from: https://pythonhosted.org/mrjob/guides/quickstart.html#writing-your-first-job\n",
    "# It's a different type of word counts.\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        yield \"chars\", len(line)\n",
    "        yield \"words\", len(line.split())\n",
    "        yield \"lines\", 1\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFrequencyCount.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# November 14\n",
    "\n",
    "### Map reduce cont.\n",
    "Lets say we want to use Map Reduce to compute matrix multiplication $ A \\times B = C $ where $C_{i,j} = A_{i,0}\\times B_{0,j} + A_{i,1} \\times B_{1,J} $\n",
    "\n",
    "We've got a set of inputs like: $A_{0,0} = 22$ (Or: Key=(A,0,0), value=22), and we want to produce output like $C_{0,0}=32$.\n",
    "\n",
    "Input: (no key), (A,1,1,3) -> Map -> (C,1,1), [((A,1,1),3)...] -> Reduce -> (C,1,1), 32\n",
    "\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRMatrixMultiply(MRJob):\n",
    "\n",
    "    def mapper(self, _, matrix_point):\n",
    "        # matrix_point is in the form of ('Matrix', row, col, value)\n",
    "        if(matrix_point[0] == 'A'):\n",
    "            for i in range(C_HEIGHT):\n",
    "                yield ('C', matrix_point[1], i), matrix_point\n",
    "        else: # matrix_point[0] == 'B'\n",
    "            for i in range(C_WIDTH):\n",
    "                yield ('C', i, matrix_point[2]), (matrix_point[0:3],matrix_point[3])\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        inputs = dict(values)\n",
    "        value = 0\n",
    "        for i in range(A_WIDTH):\n",
    "            value += inputs[('A',key[1],i)] * inputs[('B', i, key[2])]\n",
    "            \n",
    "        # Produces ('C', row, col), value\n",
    "        yield key, value\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMatrixMultiply.run()\n",
    "```\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "#### Hunt's algorithm:\n",
    "* Let D_t be the set of training records that reach a node t.\n",
    "* Procedure:\n",
    "    1. If D_t contains recorts that belong to the same class as y_t, then t is a leaf node as y_t (If D_t is \"Pure\")\n",
    "    2. If D_t is an empty set, then t is a leaf node labeled by the default class, y_d\n",
    "    3. If D_t contains records that belong to more than one class, use an attribute test to split the data into smaller subsets. Recursively apply the procedure to each subset.\n",
    "    \n",
    "With Nominal Attributes (discrete, like color), we can have *binary* or *multi-way* split.\n",
    "For continuous attributes, we can do similar\n",
    "\n",
    "#### How to determine the best split:\n",
    "Nodes with **homogeneous** class distribution are preferred.\n",
    "We need a measure of *node impurity*.\n",
    "\n",
    "* Entropy: $-p \\log_2(p) - q \\log_2(q) $ (Lower is better, more categories is fine)\n",
    "* Gini index: used in economics $ GINI(t) = 1-\\sum_j [P(j|t)]^2$ (Lower is better)\n",
    "* Clasification error: $ Error(t) = 1-max(P(i|t)) $ (Again, lower is better)\n",
    "\n",
    "### Bagging/Bootstrap aggrigation\n",
    "Averaging over many trees.\n",
    "\n",
    "1. Bootstrap Sampling: Uniform sampling (of the training data) with replacement\n",
    "2. Train a tree on each bootstrap sample\n",
    "3. Prediction: vote (classification) / mean prediction (regression)\n",
    "General idea: average many noisy but approximately unbiased models to make our predictions more accurate.\n",
    "\n",
    "**Random forests** - Give a few attributes to different trees (and also different training data), and then get the average of all the trees. (Often better than Bagging)\n",
    "\n",
    "**Gradient Boosting** - Use any number of techniques that use bagging/forests, but instead of performing uniform sampling select datapoints with errors so that we train each tree/model to shore up the weaknesses of the previous ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# November 17\n",
    "\n",
    "- **Imitating expert moves** - Supervised learning\n",
    "- **Improving through self-plays** - REinforcement learning\n",
    "\n",
    "\n",
    "### Neural networks\n",
    "\n",
    "**Gradient descent**\n",
    "\n",
    "We can calculate the success of a NN with $Error(y_{predicted}, y_{actual}) = (y_{predicted}-y_{actual})^2$\n",
    "\n",
    "And we want to minimize the error, so we do gradient descent.\n",
    "Assuming we have two weights ($w_1$ and $w_2$)\n",
    "\n",
    "We update the weights $W_new = W_old + \\Delta W $ where\n",
    "$\\Delta w = r \\times (\\frac{\\delta P}{\\delta w_1}) $\n",
    "\n",
    "$r$ is the learning rate (higher trains faster, is less accurate), around 0.1 is typical.\n",
    "\n",
    "$\\delta P$ is the derivative of ...\n",
    "\n",
    "If Neuron of W_1 -> W_2 -> output\n",
    "\n",
    "deltaP/deltaPrediction * deltaPrediction/delta W_2 * delta "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monday November 21\n",
    "### Neural networks\n",
    "\n",
    "The [**Mark I Perceptron**](https://en.wikipedia.org/wiki/Perceptron) machine was the first implementation of the perceptron algorithm - connected to a camera that used 20x20 cadmium sulfide photocells. (Around 1957)\n",
    "\n",
    "Nonlinear activation functions:\n",
    "\n",
    "* Tanh - $f(x) = e^x-e^{-x} /(e^x+e^{-x})$\n",
    "* Sigmoid (logistic) - $f(x) = e^x  / (1+ e^x)$\n",
    "* Rectifier (Rectified Linear Unit, or ReLU) - `f(x) := (x < 0) ? 0 : x;`\n",
    "* ??? - `f(x) := (x < 0);` - usually not picked because it doesn't play well with back propagation.\n",
    "\n",
    "### Reinforcement learning\n",
    "Train an \"Agent\" to make certain actions by allowing it to experiment, and rewarding it if it does well.\n",
    "\n",
    "The Agent has access to a world state (which may be represented in many different ways)\n",
    "\n",
    "Policy function: Function that goes from World State -> Action\n",
    "\n",
    "\n",
    "Training for reinforcement learning is very similar to classification problem training, except that you don't get feedback until the task is complete.\n",
    "We can think of each \"game\" or task as an epoch.\n",
    "\n",
    "### AutoEncoder\n",
    "\n",
    "Has an encoder NN and a decoder NN.\n",
    "\n",
    "The input is very high dimensional, but we want to encode it in very few dimensions. We can think of the auto encoder as a NN that is trained to produce the same output as input, but somewhere in the middle, there are far too few neurons to fully represent the data.\n",
    "\n",
    "Can be trained layer by layer (e.g. Train encoding one layer, then reversing it. Once that's trained, add another layer)\n",
    "\n",
    " *You can use CNNs as auto-encoders! (they're called convolutional encoders)*\n",
    "\n",
    "### Convolutional Neural Networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monday November 28\n",
    "\n",
    "### Recurrent Neural Networks\n",
    "Variable length, uses sequences and often words or characters.\n",
    "\n",
    "##### How to represent a text document?\n",
    "- **Bag-of-words**: Represetn documents as a vector of `vocabulary` length, with each value being how often the word occurs. Not great because it ignores positional data\n",
    "    - **Continuous bag of words**- A list of one-hot input vectors (1 per word)\n",
    "        - Skip-gram - use continuous-bag-of-words to predict surrounding words.\n",
    "            - The intermediate internal representation of a word (AKA word vector) is informative (Man - King is similar to Woman - Queen)\n",
    "- **Encode words** - Could do hashing function(s), could do ascii values up to a full length, one-of-N encoding (one-hot), ...\n",
    "\n",
    "Okay, once we encode the words, we want to combine them to analyze the document.\n",
    "\n",
    "We use recurrent neural networks RNNs to process.\n",
    "\n",
    "###### RNNs: a NN that has:\n",
    "* Inputs\n",
    "    * A vector representing previous words (hidden state)\n",
    "    * A word as a word vector or as a one-hot encoding (or just a character)\n",
    "* Outputs\n",
    "    * A prediction of the next input word (or character)\n",
    "    * A vector representing previous words (hidden state)\n",
    "\n",
    "This gives us a unique *hidden state* for our words.\n",
    "There are many different structures of RNNs.\n",
    "\n",
    "Can give an RNN a seed sequence.\n",
    "\n",
    "\n",
    "#### LSTM: Long Short term memory\n",
    "Take previous memory and feed into next memory\n",
    "\n",
    "$f_t = \\sigma(W_i\\cdot [h_{t-1}, x_t] + b_i) $\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# December 1\n",
    "\n",
    "### Cluster analysis\n",
    "##### Algorithms:\n",
    "* k-means\n",
    "* DBSCAN\n",
    "* Hierarchical clustering\n",
    "\n",
    "##### Uses\n",
    "* Summarizing (e.g. lets you look at at a few examples of each cluster instead of each datapoint, like customers and their shopping habits)\n",
    "* Information Retrieval (e.g. find similar datapoints, like webpages, or, get one of each type)\n",
    "* In biology, we could group genes or group individuals, or brain scan data.\n",
    "* Data compression (e.g. compress an image's pixels into 10 groups)\n",
    "\n",
    "\n",
    "#### Goal of clustering\n",
    "The best clustering minimizes or maximizes an objective function, e.g. minimize mean squared distance from each cluster's center. `O(k)* (O(n)(for centering) + O(n)(summing distance)`\n",
    "\n",
    "Similarity\n",
    "\n",
    "Getting the *best* cluster centers is an NP hard problem.\n",
    "\n",
    "#### Types of clustering algorithms\n",
    "* Parition based\n",
    "* Hierarchical - smaller clusters belong to larger clusters, which belong to larger clusters - e.g. each point has one of its nearest neighbor in it's cluster, each of those clusters are clustered with another...\n",
    "\n",
    "#### K-means\n",
    "Partition based clustering\n",
    "\n",
    "We want to minimize $\\Sigma_k \\Sigma (x-c)$ where $k = \\text{number of clusters,} x = \\text{data point,} c = \\text{cluster center}$\n",
    "\n",
    "\n",
    "Randomly initialize the centroids, assign all points to their nearest centroid (like voronoi diagram).\n",
    "\n",
    "Here's some psuedocode:\n",
    "```python\n",
    "k = clusters\n",
    "X = input_points\n",
    "centroids = [ X.random_sample() for i in range(k) ]\n",
    "\n",
    "while centroids changes:\n",
    "    for x in X:\n",
    "        x.nearest(centroids).X_values.append(x)\n",
    "    \n",
    "    centroids = [ c.X_values.avg() for c in centroids ]\n",
    "```\n",
    "\n",
    "##### limitations of K-means clustering\n",
    "\n",
    "Because the centroids are randomly selected, we may not come to an optimal solution.\n",
    "To fix this we could:\n",
    "\n",
    "* Run multiple times\n",
    "* Use hierarchical clustering to determine initial centroids \n",
    "* Select more than k intitial centroids, and then pick the k farthest apart of the initial centroids.\n",
    "\n",
    "K-means also doesn't handle clusters of differing size (clusters tend to be of similar size or densities (tends to split less dense clusters), or odd shapes (prefers data that has convex linear borders).\n",
    "Can fix this by using a larger number of clusters, where several clusters represent a real cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# December 5\n",
    "### Clustering continued\n",
    "\n",
    "### Hierarchical clustering\n",
    "K-means (or kMeans) clustering will not give a good result if the K parameter is wrong.\n",
    "\n",
    "Hierarchical clustering avoids this issue.\n",
    "\n",
    "\"Putting the Dendrogram at the proper level\" - Dendrogram = tree diagram\n",
    "\n",
    "There are two approaches to Hierarchical Clustering:\n",
    "\n",
    "- Agglomerative - Start with leaf nodes, and combine until we get one cluster\n",
    "- Divisive - Start with one cluster, and then divide each cluster into two.\n",
    "\n",
    "#### Agglomerative clustering\n",
    "1. Computer the proximity matrix\n",
    "2. Let each data point be a cluster\n",
    "3. Repeat\n",
    "    1. Merge the two closest clusters\n",
    "    2. Update the proximity matrix\n",
    "    3. Repeat until there's only one cluster left\n",
    "\n",
    "There are multiple different ways to compute similarity of two clusters - distance between centers is only one way.\n",
    "\n",
    "- Center distance: **Pros:** Quick to compute, simple **Cons:** Spherical, ...\n",
    "- Minimum distance: **Pros:** Non-spherical, non-convex clusters. **Cons:** Chaining\n",
    "- Max distance (Complete link): **Pros:** Robust against noise **Cons:** Breaks large clusters, biased towards globular clusters\n",
    "- Group Average: (average pairwise distance) - compromise between min and max. **Pros:** **Cons:**\n",
    "\n",
    "\n",
    "$O(N^2)$ space because of the proximity matrix.\n",
    "$O(N^3)$ time in many cases - can be reduced to $O(N^2 log(N))$\n",
    "\n",
    "#### Limitations:\n",
    "- Greedy: Once a decision is made to combine two clusters, it cannot be undone\n",
    "- No global objective function is directly minimized\n",
    "- Different distance schemes have problems with one or more of the following:\n",
    "    - Sensitivity to noise & outliers\n",
    "    - Difficulty handling oddly sized structures\n",
    "    - ...\n",
    "\n",
    "Here's some inefficient psuedocode (that avoids similarity matrix):\n",
    "```python\n",
    "clusters = [Cluster(x) for x in X]\n",
    "\n",
    "# O(n^3) or O(n^3*log(n)) depending if center is computed efficiently\n",
    "while len(clusters) > 1:\n",
    "    min_threshold = float('inf')\n",
    "    c1 = None\n",
    "    c2 = None\n",
    "    for cluster in clusters:\n",
    "        for cluster2 in cluster:\n",
    "            distance = cluster.distance(cluster2)\n",
    "            if distance < min_threshold:\n",
    "                c1 = cluster\n",
    "                c2 = cluster2\n",
    "                min_threshold = distance\n",
    "\n",
    "    clusters.remove(c1)\n",
    "    clusters.remove(c2)\n",
    "    clusters.append(Cluster([c1, c2]))\n",
    "\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(data):\n",
    "        if data isinstance point:\n",
    "            self.center = data\n",
    "            self.n_points = 1\n",
    "        else:\n",
    "            self.n_points = data[0].n_points + data[1].n_points\n",
    "            self.center = (data[0].n_points*data[0].center + data[1].n_points*data[1].center) / self.n_points\n",
    "   \n",
    "    def distance(other):\n",
    "       return euc_dist(this.center, other.center)\n",
    "```\n",
    "More efficient psuedocode:\n",
    "```python\n",
    "\n",
    "# Still O(n^3), but faster constant term\n",
    "# => (computing a single row of the similarity matrix, not the whole thing. Still does argmin() which is O(n^2) n times\n",
    "\n",
    "clusters = [Cluster(x) for x in X]\n",
    "\n",
    "# Compute similarity matrix\n",
    "similarity_matrix = numpy.empty((len(clusters),len(clusters)))\n",
    "similarity_matrix[:] = float('infinity')\n",
    "\n",
    "for i in range(len(clusters)):\n",
    "    for j in range(i-1):\n",
    "        similarity_matrix[i,j] = clusters[i].distance(clusters[j])\n",
    "\n",
    "while len(clusters) > 1:\n",
    "    argmin = unravel_index(similarity_matrix.argmin(), similarity_matrix.shape)\n",
    "    c1 = clusters[argmin[0]]\n",
    "    c2 = clusters[argmin[1]]\n",
    "    clusters.remove(argmin[0])\n",
    "    clusters.remove(argmin[1])\n",
    "    similarity_matrix.remove_rows(argmin)\n",
    "    similarity_matrix.remove_cols(argmin)\n",
    "    new_cluster = Cluster(c1, c2)\n",
    "    similarity_matrix.appendRow([new_cluster.distance(clusters[i]) for i in range(len(clusters))])\n",
    "    clusters.append(new_cluster)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### More Clustering!\n",
    "Clustering can be used for Object identification in Lidar/Kinect and things like that.\n",
    "\n",
    "Ideal clustering would figure out the ideal number of clusters, shape of clusters, and identify outliers\n",
    "\n",
    "#### DBSCAN\n",
    "Density = Number of points within a specified radius ($\\epsilon$). This metric may also be called epsilon neighbors\n",
    "\n",
    "Density Connection - -we set a minimum density for a given $\\epsilon$ distance,\n",
    "\n",
    "1. Label points as core, border, and noise.\n",
    "    - Core points have enough ($>MinPts$) neighbors within $\\epsilon$ distance\n",
    "    - Border points are neighbors of core points\n",
    "    - Noise points are all the rest\n",
    "2. Eliminate noise points\n",
    "3. For every core point that has not been assigned to a cluster\n",
    "    - Create a new cluster wit hthe point p and all the (core) points that are density-connected to p.\n",
    "4. Assign border points to the cluster of the closest core point.\n",
    "\n",
    "##### Choosing good parameters:\n",
    "- For points in a cluster, their *k*th nearest neighbors should be at roughly the same distance.\n",
    "- Noise points have the kth nearest neighbor at farther distance\n",
    "- Can plot distance to *k*th nearest neighbors (to points sorted by this value) for a given *k*\n",
    "\n",
    "##### DBSCAN limitations:\n",
    "- Varying densities\n",
    "- High-dimensional data\n",
    "\n",
    "Alternate Pseudocode (rescursive, depth first search):\n",
    "```python\n",
    "epsilon = 4.5\n",
    "min_density = 3\n",
    "\n",
    "# Compute the density of all points\n",
    "\n",
    "for x in X:\n",
    "    x.neighbors = []\n",
    "    for x2 in X:\n",
    "        if(distance(x, x2) < epsilon):\n",
    "            x.neighbors.append(x2)\n",
    "\n",
    "# Grows a cluster recursively\n",
    "def add_to_cluster(x):\n",
    "    if len(x.neighbors) < min_density: # Border points\n",
    "        return\n",
    "    for x1 in x.neighbors:\n",
    "        if x1.cluster != cluster:\n",
    "            if x1.has_cluster():\n",
    "                \n",
    "            x1.cluster = x.cluster\n",
    "            add_to_cluster(x1)\n",
    "    \n",
    "for x in X:\n",
    "    if x.cluster != None:\n",
    "        continue\n",
    "    if len(x.neighbors) < epsilon:\n",
    "        continue # Border point/noise\n",
    "    \n",
    "    x.cluster = Cluster() # create a new cluster\n",
    "    add_to_cluster(x) # and grow it\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# December 8\n",
    "\n",
    "### Examples of graph clustering problems:\n",
    "* **Social Networks**: Groups friends by where they were made\n",
    "* **Web**: What are similar web pages?\n",
    "* Linux source code function calls\n",
    "* Gene Regulation network (E.Coli was the example)\n",
    "\n",
    "### How do we do graph clustering?\n",
    "Lets set up an adjacency matrix.\n",
    "If there are N items, create an N by N matrix, it is a binary, symmetric matrix.\n",
    "E.G:\n",
    "```\n",
    "[[0, 1, 1],\n",
    " [1, 0, 0],\n",
    " [1, 0, 0]]\n",
    "```\n",
    "Let's try something:\n",
    "\n",
    "1. We can have $Ax=y$, where $A$ is the adjacency matrix, $x$ is the labels of each of the points, and $y$ is the sum of all the neighbor's labels.\n",
    "\n",
    "2. What if $Ax=\\lambda x$ where $\\lambda$ is a constant. (Technically, $x$ becomes an eigenvector and $\\lambda$ becomes an eigenvalue)\n",
    "\n",
    "3. For $Ax=\\lambda x$, if $x$ is all ones and $A$ is fully connected (all ones, except for a line down the diagonal), then $\\lambda = length(x)$ \n",
    "\n",
    "4. It's taking a while to get to the point.\n",
    "\n",
    "5. If we have two triangles, and we want a specific $\\lambda$ in $Ax=\\lambda x$:\n",
    "    - A = [[011000], [101000], [110000], [000011], [000101], [000110]]\n",
    "    - if $\\lambda$ = 0, then $x = <0,0,0,0,0,0>$\n",
    "    - if $\\lambda$ = 2, then $x = <1,1,1,1,1,1>$, or x can equal $x = <1,1,1,0,0,0>$, or $x = <0,0,0,1,1,1>$\n",
    "    - Similar things hold for more complex fully-connected clusters... (e.g. 3 groups of 4 points that are all connected to each other can have a $\\lambda$ of 3)\n",
    "\n",
    "6. Now lets say we have a triangle and fully connected set of 4 points (tetragon):\n",
    "    - Let $x$ be $<tri, tri, tri, tet, tet, tet, tet>$\n",
    "    - if $\\lambda$ = 2, then $x = <1,1,1,0,0,0,0>$\n",
    "    - if $\\lambda$ = 3, then $x = <0,0,0,1,1,1,1>$\n",
    "\n",
    "7. In these cases, the $\\lambda$ represents the points that have $\\lambda$ edges.\n",
    "    - $\\lambda$ = Degree Distribution, $x$ = compoonents/partition (ignore zeros)\n",
    "\n",
    "8. So, we know how to get the eigenpairs for the matrix $A$ (see earlier homeworks)\n",
    "\n",
    "9. But this doesn't work terribly well for all clusters\n",
    "    - It requires each cluster to be unconnected to other clusters\n",
    "    - It requires all clusters to be fully connected\n",
    "    \n",
    "    \n",
    "So now that we've done all that work with the $A$ Matrix, lets make a new one. It'll be fun, right!\n",
    "\n",
    "The **Degree Matrix** ($D$), is a diagonal matrix with the diagnonal values set to the number of connections of each point, and the rest are 0.\n",
    "\n",
    "$Dx = y$ gives us a $y$ that is $y[i] = x[i] * D[i][i]$ which is $y[i] = \\text{point i's connections} * \\text{point i's label}$\n",
    "\n",
    "**Laplacian Matrix** $L$ is $L = D - A$, (has a lot of -1s, 0s, and positive values on the diagonal)\n",
    "\n",
    "Now, let's say we've got $Lx = y$ ($Lx = Dx - Ax$)\n",
    "\n",
    "But we've been having fun with EigenPairs, so let's figure out what the eigenpairs of $L$ mean.\n",
    "$$L x = \\lambda x$$\n",
    "\n",
    "- Eigen pair 1: $\\lambda = 0$, $x = <1,1,1,1,1,1>$\n",
    "- Eigen pair 2 of any laplacian:\n",
    "    - For any symmetrix matrix M, $\\lambda_2 = min_x \\frac{x^T M x}{x^T x})$ ($M = L$)\n",
    "    - What is the meaning of $x^T M x$?\n",
    "        - If we go back to the Adjacency matrix, $x^T A x$ = sum of the links in x's partition * 2 (we count both sides) (Note: $x$ consists of 0s and 1s [and is a binary partition])\n",
    "        - Now let's use $D$: $x^T D x $ = sum of *all* the links of the points in x's partition (not just the links with both sides in x)\n",
    "        - $x^T D x  - x^T A x = x^T L x$ = sum of the links between points in x's partition and those *not* in x's partition (aka the number of edges cut by the partitioning)\n",
    "    - Incidently $x^T x$ = nodes in the partition.\n",
    "\n",
    "\n",
    "If we minimize $\\frac{x^T L x}{x^T x}$ (connections-between-clusters/size-of-cluster (also, the first non-zero eigenvalue of $L$), we get a good clustering split.\n",
    "\n",
    "\n",
    "So... here is the useful process we get out of this:\n",
    "#### Binary clustering\n",
    "1. Compute L = D - A\n",
    "2. Compute Eigen pairs of Matrix L\n",
    "3. Use the smallest Eigen vector with non-zero eigen values\n",
    "4. Use zero to cut.\n",
    "\n",
    "\n",
    "#### Spectral (non-binary clustering)\n",
    "1. Compute $L = D - A$\n",
    "2. Compute Eigen pairs of Matrix $L$\n",
    "3. Sort first k Eigen vectors with non-zero Eigen values\n",
    "4. Perform k-Means on these Eigen vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# December 12\n",
    "### Spectral clustering cont.\n",
    "Can do spectral clustering on traditional datasets - need to convert points to a (symmetric) graph.\n",
    "Just find the k-Nearest neighbors (though it has to be symmetric) for all the points, and combine them.\n",
    "\n",
    "\n",
    "## PageRank\n",
    "\n",
    "HITS Algorithm, PageRank are two similar old algorithms for page ranking.\n",
    "\n",
    "\n",
    "###### Evaluatting nodes (pages) on the web\n",
    "* **Degree Centrality:** Number of edges pointing to a node (Pros: Is generally an active player in the network, Cons: Ignores the whole graph)\n",
    "\n",
    "* **Closeness Centrality:** The inverse of the average length of all its shortest paths\n",
    "    - Pros: Is close to other entities\n",
    "    - Cons: Ignore the importance of different nodes, easy to spoof (creating lots of dummy pages)\n",
    "    - $1/\\sum_N(shortest\\_path(N_{evaluating}, N_i))$ (Trying to evaluate a Node in N.\n",
    "    \n",
    "* **Betweenness Centrality:** Fraction of shortest paths that pass through the Node we're evaluating.\n",
    "    - Pros: Represents a signle point of failure, has a greater amount of influence over what happens in network\n",
    "    - Cons: Ignores the importance of different nodes\n",
    "* **Eigenvalue:** Eigenvalue measures how close an entity is to other highly valued pages\n",
    "    - Harder to hack.\n",
    "\n",
    "##### Basic idea of page rank:\n",
    "\n",
    "```python\n",
    "for i in A.links:\n",
    "    i.importance += A.importance * len(A.links)\n",
    "\n",
    "```\n",
    "But, A.importance can't be calculated without figuring out what everything else's importance is so we have a **chicken and egg** problem.\n",
    "\n",
    "To solve this, we use **Random Walk**\n",
    "\n",
    "### Random Walk\n",
    "```python\n",
    "# We do this a bunch in parallel\n",
    "for _ in OTHER_BIG_NUMBER:\n",
    "    A = random_node()\n",
    "    for i in BIG_NUMBER:\n",
    "        A.importance += 1/BIG_NUMBER\n",
    "        A = random_select(A.links)\n",
    "```\n",
    "A directed graph has an Adjacency matrix ($A$) that isn't symmetric.\n",
    "\n",
    "We can convert the Adjacency matrix $A$ to the Transition matrix $P$, where $P_{ij}$ is the probablity of randomly walking from node j to node i.\n",
    "So:\n",
    "```python\n",
    "P = np.zeros_like(A)\n",
    "for i in len(A): # Rows\n",
    "    for j in len(A): # Columns\n",
    "        P[i,j] = A[i,j] / sum(A[:, j])\n",
    "```\n",
    "And with current location `loc = <0,0,0,1>`,\n",
    "`next_loc_possibilities = P*current_loc`\n",
    "\n",
    "So to do the code at the top of this section more efficiently:\n",
    "```python\n",
    "starting_location = one_hot_vector(random(len(P))\n",
    "cur_loc = starting_location\n",
    "#for i in NUMBER_OF_STEPS:\n",
    "while cur_loc != old_cur_loc:\n",
    "    old_cur_loc = cur_loc\n",
    "    cur_loc = P * cur_loc\n",
    "# when this converges, we get an Eigenvector with an eigen value of 1!    \n",
    "```\n",
    "\n",
    "But why start with only one position?\n",
    "```python\n",
    "P = np.zeros_like(A)\n",
    "for i in len(A): # Rows\n",
    "    for j in len(A): # Columns\n",
    "        P[i,j] = A[i,j] / sum(A[:, j])\n",
    "\n",
    "starting_probability = np.ones(len(P))\n",
    "starting_probability[:] = 1/len(P)\n",
    "cur_p = starting_probability\n",
    "for i in NUMBER_OF_STEPS:\n",
    "    old_p = cur_p\n",
    "    cur_p = P * cur_locs\n",
    "\n",
    "# Cur p represents the chance to be at each position after the walker walked NUMBER_OF_STEPS\n",
    "```\n",
    "#### Issues with a naive Random Walk\n",
    "###### Sink Nodes\n",
    "Nodes with no links out kill some our probabilities, which can create 0 vectors which aren't helpful.\n",
    "\n",
    "(Also, if no one links to a page, it is of No importance, but that's okay.)\n",
    "\n",
    "Nodes with no links out are called **sink nodes**.\n",
    "To prevent them from being a problem, link to *all* the other nodes (and itself) equally.\n",
    "\n",
    "###### Non-convergence\n",
    "There are graphs that will never converge (e.g. a big circle shaped graph)\n",
    "\n",
    "###### Sink Regions\n",
    "If there are regions that link never link to other regions, we can \n",
    "\n",
    "###### Solution to non-convergence and sink regions:\n",
    "G = a*S + (1-a)/n * np.ones(n,n)\n",
    "(1-a = tax rate)\n",
    "\n",
    "The tax rate effects the standard deviations of the final tax rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def determinant(m):\n",
    "    if(m.shape != (m.shape[0], m.shape[0])):\n",
    "        print \"Can't get a determinant from a non-square matrix\"\n",
    "    det = 0\n",
    "    for i in range(m.shape(0)):\n",
    "        # Get all but m[0][i]'s row and column\n",
    "        selector = [x for x in range(m.shape[1]) if x != i]\n",
    "        det += -1**i * m[0][i] * det(m[1:, selector])\n",
    "    return det;\n",
    "\n",
    "\n",
    "def eigenVectors(A):\n",
    "    if(A.shape != (A.shape[0], A.shape[0])):\n",
    "        printf(\"Not a square matrix!\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def decorate(q):\n",
    "    def q1():\n",
    "        y = q()\n",
    "        print(y)\n",
    "        return y\n",
    "    return q1\n",
    "\n",
    "@decorate\n",
    "def x():\n",
    "    return 1\n",
    "\n",
    "x()\n",
    "x()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
